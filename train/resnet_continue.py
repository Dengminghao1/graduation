import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm
import matplotlib.pyplot as plt

from train.resnet_face import criterion

# é…ç½®ä¸ä¹‹å‰ä¿æŒä¸€è‡´
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
data_dir = r"/home/ccnu/Desktop/dataset/classified_frames_by_label_all"
batch_size = 256
num_epochs = 50
learning_rate = 0.00005  # ç»­è®­æ—¶å»ºè®®ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼Œæˆ–è€…ä¿æŒä¹‹å‰çš„
num_classes = 5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ç»­è®­ä¸“ç”¨å‚æ•° ---
RESUME_MODEL = 'best_model_acc_8487.pth'  # ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶å
START_EPOCH = 50  # å‡è®¾ä¹‹å‰è·‘äº†50è½®

# 1. æ•°æ®å‡†å¤‡ (ä¸ä¹‹å‰ä¸€è‡´)
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

full_dataset = datasets.ImageFolder(data_dir)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2, stratify=full_dataset.targets, random_state=42
)


class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform

    def __getitem__(self, index):
        x, y = self.subset[index]
        if self.transform: x = self.transform(x)
        return x, y

    def __len__(self): return len(self.subset)


train_loader = DataLoader(ApplyTransform(Subset(full_dataset, train_idx), data_transforms['train']),
                          batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(ApplyTransform(Subset(full_dataset, val_idx), data_transforms['val']),
                        batch_size=batch_size, shuffle=False, num_workers=4)

# 2. æ„å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
model = models.resnet50(weights=None)  # ç»­è®­ä¸éœ€è¦é‡å¤ä¸‹è½½ ImageNet æƒé‡
num_ftrs = model.fc.in_features
model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, num_classes))

if os.path.exists(RESUME_MODEL):
    print(f"ğŸš€ æ­£åœ¨ä» {RESUME_MODEL} æ¢å¤è®­ç»ƒ...")
    model.load_state_dict(torch.load(RESUME_MODEL, map_location=device))
else:
    print("âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
    exit()

model = model.to(device)

# 3. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)
scaler = GradScaler()

# 4. è®­ç»ƒå¾ªç¯ (åŠ å…¥æ—©åœå’Œå†å²è®°å½•)
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
best_val_acc = float(RESUME_MODEL.split('_')[-1].split('.')[0]) / 10000.0  # ä»æ–‡ä»¶åè§£æå‡ºä¹‹å‰çš„ Acc
patience_counter = 0
early_stop_patience = 10

for epoch in range(START_EPOCH, START_EPOCH + num_epochs):
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0

    for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # 4090 æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡
        running_loss += loss.item() * inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            inputs, labels = inputs.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                v_loss = criterion(outputs, labels)

            val_loss += v_loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val
    scheduler.step(epoch_val_loss)
    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')
    # --- å»ºè®®å¢åŠ ï¼šæ—©åœæœºåˆ¶ (Early Stopping) ---
    # é˜²æ­¢åé¢ 20 ä¸ª epoch éƒ½åœ¨æµªè´¹ç”µå¹¶åŠ å‰§è¿‡æ‹Ÿåˆ
    # --- 3. ä¿å­˜æœ€ä½³æ¨¡å‹ (æ–‡ä»¶åä¸è¦ 0.) ---
    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc
        patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
        # æ¸…é™¤æ—§çš„ best æ¨¡å‹
        for old_file in glob.glob("best_model_acc_*.pth"):
            os.remove(old_file)

        # è½¬æ¢å‡†ç¡®ç‡ä¸ºæ•´æ•°ï¼Œå¦‚ 0.9542 -> 9542
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_model_acc_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")

    else:
        patience_counter += 1
        print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

    # è§¦å‘æ—©åœ
    if patience_counter >= early_stop_patience:
        print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
        break
    # --- 4. ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
