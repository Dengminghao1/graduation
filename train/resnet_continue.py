import glob

import torch
import torch.nn as nn
import torch.optim as optim
from matplotlib import pyplot as plt
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm

# --- 1. é…ç½®å‚æ•° (æ ¹æ® 24G æ˜¾å­˜ä¼˜åŒ–) ---
data_dir = r"/home/ccnu/Desktop/2021214387_å‘¨å©‰å©·/total/classified_frames"
batch_size = 256
start_epoch = 20  # è®°å½•ä»ç¬¬ 21 è½®å¼€å§‹
total_epochs = 60  # ç›®æ ‡æ€»è½®æ•°å»ºè®®è®¾ä¸º 60
learning_rate = 0.0001  # ç»­è®­å»ºè®®å­¦ä¹ ç‡å‡å° 10 å€ï¼Œè¿›è¡Œå¾®è°ƒ
num_classes = 5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. æ•°æ®å¢å¼º (ä¿æŒä¸å˜) ---
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

# --- 3. åŠ è½½æ•°æ® (ä¿æŒä¸å˜) ---
full_dataset = datasets.ImageFolder(data_dir)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2, stratify=full_dataset.targets, random_state=42
)


class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform

    def __getitem__(self, index):
        x, y = self.subset[index]
        if self.transform: x = self.transform(x)
        return x, y

    def __len__(self):
        return len(self.subset)


train_loader = DataLoader(ApplyTransform(Subset(full_dataset, train_idx), data_transforms['train']),
                          batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)
val_loader = DataLoader(ApplyTransform(Subset(full_dataset, val_idx), data_transforms['val']),
                        batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)

# --- 4. æ„å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡ (å…³é”®ä¿®æ”¹) ---
model = models.resnet50(weights=None)  # ä¸å†éœ€è¦ä¸‹è½½å®˜æ–¹é¢„è®­ç»ƒæƒé‡
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, num_classes)

# åŠ è½½ä½ ä¹‹å‰è®­ç»ƒå¥½çš„æƒé‡
weight_path = 'best_resnet_model.pth'
if os.path.exists(weight_path):
    print(f"æ­£åœ¨åŠ è½½å·²æœ‰æƒé‡: {weight_path}")
    model.load_state_dict(torch.load(weight_path))
else:
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œå°†ä»é›¶å¼€å§‹è®­ç»ƒï¼")

model = model.to(device)

# --- 5. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ (å¢åŠ  Scheduler) ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
# å¦‚æœæ˜¯ç»­è®­ï¼Œå»ºè®®åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ï¼›å¦‚æœä½ æœ‰ä¹‹å‰çš„å†å²æ•°æ®ä¹Ÿå¯ä»¥åœ¨æ­¤åŠ è½½
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

# æ··åˆç²¾åº¦ç¼©æ”¾å™¨ï¼ˆRTX 4090 å¿…å¤‡ï¼‰
scaler = GradScaler()

# è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)

# --- 2. è®­ç»ƒå¾ªç¯ ---
best_acc = 0.5504  # åˆå§‹æœ€ä½³å‡†ç¡®ç‡
total_epochs = 15  # ä¸¾ä¾‹
start_epoch = 0  # å¦‚æœä»å¤´å¼€å§‹æ˜¯0ï¼Œå¦‚æœæ˜¯ç»­è®­éœ€æ‰‹åŠ¨æŒ‡å®šæˆ–ä»checkpointè¯»å–

print(f"å¼€å§‹ç»­è®­... ç›®æ ‡ Epochs: {total_epochs}, å½“å‰æœ€ä½³ Acc: {best_acc:.4f}")

for epoch in range(start_epoch, total_epochs):
    # --- è®­ç»ƒé˜¶æ®µ ---
    model.train()
    train_running_loss = 0.0
    train_corrects = 0
    train_total = 0

    # ä½¿ç”¨ tqdm åŒ…è£…å¹¶æ˜¾ç¤ºå½“å‰ Epoch ä¿¡æ¯
    train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{total_epochs} [Train]")

    for inputs, labels in train_bar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # 4090 æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡ä¿¡æ¯
        batch_size = inputs.size(0)
        train_running_loss += loss.item() * batch_size
        _, preds = torch.max(outputs, 1)
        train_corrects += torch.sum(preds == labels.data)
        train_total += batch_size

        # æ›´æ–° tqdm å³ä¾§ä¿¡æ¯
        train_bar.set_postfix(loss=f"{loss.item():.4f}")

    epoch_train_loss = train_running_loss / train_total
    epoch_train_acc = train_corrects.double() / train_total

    # --- éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_running_loss = 0.0
    val_corrects = 0
    val_total = 0

    val_bar = tqdm(val_loader, desc=f"Epoch {epoch + 1}/{total_epochs} [Val]")

    with torch.no_grad():
        for inputs, labels in val_bar:
            inputs, labels = inputs.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                v_loss = criterion(outputs, labels)

            batch_size = inputs.size(0)
            val_running_loss += v_loss.item() * batch_size
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            val_total += batch_size

            val_bar.set_postfix(v_loss=f"{v_loss.item():.4f}")

    epoch_val_loss = val_running_loss / val_total
    epoch_val_acc = val_corrects.double() / val_total

    # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler.step(epoch_val_acc)
    current_lr = optimizer.param_groups[0]['lr']

    # è®°å½•å†å²æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    # æ‰“å° Epoch æ€»ç»“
    print(f"\n[Summary] Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | "
          f"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} | LR: {current_lr}")

    # --- ä¿å­˜æœ€ä½³æ¨¡å‹ (æ–‡ä»¶åå» 0.) ---
    if epoch_val_acc > best_acc:
        best_acc = epoch_val_acc
        # æ¸…ç†æ—§çš„ best æ¨¡å‹
        for old_file in glob.glob("best_model_acc_*.pth"):
            os.remove(old_file)

        acc_suffix = int(best_acc * 10000)
        save_path = f'best_model_acc_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ æ£€æµ‹åˆ°æ›´é«˜å‡†ç¡®ç‡ï¼Œå·²ä¿å­˜æ–°æ¨¡å‹: {save_path}")

# --- 3. ç»˜åˆ¶å¹¶ä¿å­˜å­¦ä¹ æ›²çº¿ ---
plt.figure(figsize=(12, 5))

# Loss å›¾åƒ
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Acc å›¾åƒ
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc')
plt.plot(history['val_acc'], label='Val Acc')
plt.axhline(y=best_acc, color='g', linestyle='--', label='Previous Best')  # æ ‡å‡ºç»­è®­å‰çš„åŸºå‡†çº¿
plt.title('Training & Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('resume_training_results.png')
plt.show()

print(f'ç»­è®­å®Œæˆ! æœ€ç»ˆæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}')