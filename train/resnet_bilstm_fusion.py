import glob
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset, Dataset
from sklearn.model_selection import train_test_split
import os
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt

# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# --- 1. é…ç½®å‚æ•° ---
face_data_dir = r"/home/ccnu/Desktop/dataset/extracted_frames_pic/face_extracted_frames_all"  # é¢éƒ¨æ•°æ®
pose_data_dir = r"/home/ccnu/Desktop/dataset/extracted_frames_pic/pose_224_all"  # è‚¢ä½“æ•°æ®
csv_dir = r"/home/ccnu/Desktop/dataset/eeg_csv"  # æ ‡ç­¾CSVæ–‡ä»¶ç›®å½•

# face_data_dir = r"D:\dataset\frame_picture\face_extracted_frames_101"  # é¢éƒ¨æ•°æ®
# pose_data_dir = r"D:\dataset\frame_picture\pose_extracted_frames_101"  # è‚¢ä½“æ•°æ®
# csv_dir = r"D:\dataset\eeg_csv"  # æ ‡ç­¾CSVæ–‡ä»¶ç›®å½•
num_workers=0
early_stop_patience = 10
batch_size = 20  # è¿›ä¸€æ­¥å‡å°ä»¥é€‚åº”åºåˆ—è¾“å…¥
num_epochs = 100
learning_rate = 0.0001
num_classes = 5  # ä½, ç¨ä½, ä¸­æ€§, ç¨é«˜, é«˜
sequence_length = 10  # å¸§åºåˆ—é•¿åº¦ï¼ˆçª—å£å¤§å°ä¸ºåï¼‰
hidden_size = 512  # LSTM éšè—å±‚å¤§å°
num_layers = 2  # LSTM å±‚æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ---
# ResNet æ ‡å‡†è¾“å…¥æ˜¯ 224x224
# ä½¿ç”¨åŸæœ‰å‚æ•°ï¼šé¢éƒ¨å’Œè‚¢ä½“åˆ†åˆ«ä½¿ç”¨å„è‡ªçš„æ ‡å‡†åŒ–å‚æ•°
data_transforms = {
    'face_train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'face_val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'pose_train': transforms.Compose([
        transforms.Resize(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(5),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.03, 0.03),
            scale=(0.98, 1.02)
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
    'pose_val': transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
}

# --- 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» --- 
# --- MultiSegmentAttentionDataset ç±» --- 
class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_dir, seq_len=10, transform=None, segment_keys=None, is_pose=False):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform
        self.label_map = {'ä½': 0, 'ç¨ä½': 1, 'ä¸­æ€§': 2, 'ç¨é«˜': 3, 'é«˜': 4}
        self.is_pose = is_pose

        # 1. æ‰«æå¹¶åŠ è½½ CSV æ ‡ç­¾åº“
        # key: (start_time_str, end_time_str), value: DataFrame
        self.label_dfs = {}
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        print(f"æ­£åœ¨åŠ è½½ {len(csv_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶...")

        for cf in csv_files:
            # å‡è®¾ CSV æ–‡ä»¶åæ ¼å¼: xxxx_xxxx_20231229153000_20231229154000.csv
            parts = cf.replace('.csv', '').split('_')
            # æ ¹æ®ä½ çš„æ–‡ä»¶åè§„åˆ™ï¼Œå€’æ•°ç¬¬äºŒå’Œå€’æ•°ç¬¬ä¸€é€šå¸¸æ˜¯æ—¶é—´
            s_str, e_str = parts[-2], parts[-1]

            df = pd.read_csv(os.path.join(csv_dir, cf))
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            self.label_dfs[(s_str, e_str)] = df

        # 2. è§£æå›¾åƒæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ®µåˆ†ç»„
        segments = defaultdict(list)
        if self.is_pose:
            all_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]
        else:
            all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]
        print(f"æ­£åœ¨è§£æ {len(all_files)} ä¸ªå›¾åƒæ–‡ä»¶...")

        for f in all_files:
            parts = f.split('_')
            if self.is_pose:
                # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                if len(parts) >= 6:
                    frame_idx = int(parts[-2])
                    s_time_str = parts[-4]
                    e_time_str = parts[-3]
            else:
                # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                if len(parts) >= 5:
                    frame_idx = int(parts[1])
                    s_time_str = parts[-2]
                    e_time_str = parts[-1].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            total_milliseconds = frame_idx * 100  # 0.1ç§’ = 100æ¯«ç§’
            curr_dt = start_dt + timedelta(milliseconds=total_milliseconds)
            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. å¦‚æœæŒ‡å®šäº† segment_keysï¼Œåˆ™åªä¿ç•™è¿™äº›æ®µçš„æ•°æ®
        if segment_keys is not None:
            filtered_segments = {k: v for k, v in segments.items() if k in segment_keys}
        else:
            filtered_segments = segments

        # 4. æ„å»ºåºåˆ—å¹¶ä»"å¯¹åº”"çš„ DataFrame ä¸­å–æ ‡ç­¾
        self.valid_sequences = []
        print("å¼€å§‹æ—¶åºåŒ¹é…æ ‡ç­¾...")

        for seg_key, seg_files_list in filtered_segments.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ CSV æ ‡ç­¾
            if seg_key not in self.label_dfs:
                print(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°æ®µ {seg_key} å¯¹åº”çš„ CSV æ ‡ç­¾ï¼Œè·³è¿‡...")
                continue

            current_df = self.label_dfs[seg_key]
            seg_files = sorted(seg_files_list, key=lambda x: x['idx'])

            for i in range(0, len(seg_files) - seq_len + 1, 10):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # ä½¿ç”¨æœ€åä¸€å¸§çš„æ ‡ç­¾
                end_frame_time = seq_frames[-1]['time']
                time_diffs = (current_df['timestamp'] - end_frame_time).abs()
                closest_idx = time_diffs.idxmin()
                label_str = current_df.loc[closest_idx, 'attention']
                
                self.valid_sequences.append({
                    'files': [x['filename'] for x in seq_frames],
                    'label': self.label_map.get(label_str, 2)
                })

        print(f"æˆåŠŸæ„å»ºåºåˆ—æ€»æ•°: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            else:
                # å¦‚æœæ²¡æœ‰transformï¼Œè‡³å°‘è½¬æ¢ä¸ºtensor
                from torchvision import transforms
                img = transforms.ToTensor()(img)
            frames.append(img)
        # è¿”å›å¸§åºåˆ—å’Œæ ‡ç­¾
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long)


# åº”ç”¨å˜æ¢çš„æ•°æ®é›†ç±»
class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, transform=None):
        self.dataset = dataset
        self.indices = indices
        self.transform = transform

    def __getitem__(self, index):
        img_path, target = self.dataset.samples[self.indices[index]], self.dataset.targets[self.indices[index]]
        from PIL import Image
        img = Image.open(img_path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, target

    def __len__(self):
        return len(self.indices)

# --- 4. è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å™¨ --- 
class FusionDataset(torch.utils.data.Dataset):
    def __init__(self, face_data_dir, pose_data_dir, csv_dir, sequence_length=10):
        self.sequence_length = sequence_length
        self.samples = []
        self.targets = []
        
        # åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®
        face_dataset = MultiSegmentAttentionDataset(face_data_dir, csv_dir, sequence_length, is_pose=False)
        pose_dataset = MultiSegmentAttentionDataset(pose_data_dir, csv_dir, sequence_length, is_pose=True)
        
        # æŒ‰åºåŒ¹é…é¢éƒ¨å’Œè‚¢ä½“æ ·æœ¬
        matched_count = 0
        
        # éå†é¢éƒ¨åºåˆ—
        for face_seq in face_dataset.valid_sequences:
            # è·å–é¢éƒ¨åºåˆ—çš„æ—¶é—´åŒºé—´å’Œå¸§å·
            first_face_img = face_seq['files'][0]
            face_parts = first_face_img.split('_')
            if len(face_parts) >= 5:
                # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                face_interval = f"{face_parts[-2]}_{face_parts[-1].split('.')[0]}"
                face_frame_num = int(face_parts[1])
                
                # åœ¨è‚¢ä½“åºåˆ—ä¸­æŸ¥æ‰¾åŒ¹é…çš„åºåˆ—
                for pose_seq in pose_dataset.valid_sequences:
                    first_pose_img = pose_seq['files'][0]
                    pose_parts = first_pose_img.split('_')
                    if len(pose_parts) >= 6:
                        # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                        pose_interval = f"{pose_parts[-4]}_{pose_parts[-3]}"
                        pose_frame_num = int(pose_parts[-2])
                        
                        # æ£€æŸ¥æ—¶é—´åŒºé—´å’Œå¸§å·æ˜¯å¦åŒ¹é…
                        if face_interval == pose_interval and abs(face_frame_num - pose_frame_num) == 0:
                            # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
                            if len(face_seq['files']) == sequence_length and len(pose_seq['files']) == sequence_length:
                                self.samples.append((face_seq['files'], pose_seq['files']))
                                self.targets.append(face_seq['label'])
                                matched_count += 1
                                break
        
        print(f"æˆåŠŸåŒ¹é… {matched_count} å¯¹åºåˆ—æ ·æœ¬")
    
    def __getitem__(self, index):
        face_img_names, pose_img_names = self.samples[index]
        target = self.targets[index]
        
        from PIL import Image
        face_imgs = []
        pose_imgs = []
        
        # åŠ è½½åºåˆ—ä¸­çš„æ‰€æœ‰å›¾åƒ
        for face_name, pose_name in zip(face_img_names, pose_img_names):
            face_img = Image.open(os.path.join(face_data_dir, face_name)).convert('RGB')
            pose_img = Image.open(os.path.join(pose_data_dir, pose_name)).convert('RGB')
            face_imgs.append(face_img)
            pose_imgs.append(pose_img)
        
        return face_imgs, pose_imgs, target
    
    def __len__(self):
        return len(self.samples)

# åº”ç”¨å˜æ¢çš„èåˆæ•°æ®é›†ç±»
class FusionApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, face_transform=None, pose_transform=None):
        self.dataset = dataset
        self.indices = indices
        self.face_transform = face_transform
        self.pose_transform = pose_transform

    def __getitem__(self, index):
        face_imgs, pose_imgs, target = self.dataset[self.indices[index]]
        transformed_face_imgs = []
        transformed_pose_imgs = []
        
        # å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨å˜æ¢
        for face_img, pose_img in zip(face_imgs, pose_imgs):
            if self.face_transform:
                face_img = self.face_transform(face_img)
            if self.pose_transform:
                pose_img = self.pose_transform(pose_img)
            transformed_face_imgs.append(face_img)
            transformed_pose_imgs.append(pose_img)
        
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œç»´åº¦ä¸º (åºåˆ—é•¿åº¦, é€šé“, é«˜åº¦, å®½åº¦)
        transformed_face_imgs = torch.stack(transformed_face_imgs)
        transformed_pose_imgs = torch.stack(transformed_pose_imgs)
        
        return transformed_face_imgs, transformed_pose_imgs, target

    def __len__(self):
        return len(self.indices)

# --- é¢„å¤„ç†æ•°æ®è¯»å–æ•°æ®é›† --- 
class PreprocessedDataset(torch.utils.data.Dataset):
    def __init__(self, preprocessed_data_dir):
        self.preprocessed_data_dir = preprocessed_data_dir
        self.metadata = torch.load(os.path.join(preprocessed_data_dir, 'metadata.pt'))
        self.total_sequences = self.metadata['total_sequences']
        self.batch_size = self.metadata['batch_size']
        self.total_batches = self.metadata['total_batches']
        
        # é¢„åŠ è½½æ‰¹æ¬¡ç´¢å¼•ï¼Œé¿å…é‡å¤åŠ è½½æ‰¹æ¬¡æ–‡ä»¶
        self.batch_indices = []
        self.batch_sizes = []
        for batch_idx in range(self.total_batches):
            # åªåŠ è½½ä¸€æ¬¡æ‰¹æ¬¡å¤§å°ä¿¡æ¯ï¼Œé¿å…é‡å¤åŠ è½½æ•´ä¸ªæ‰¹æ¬¡
            face_batch_path = os.path.join(preprocessed_data_dir, 'face', f'face_batch_{batch_idx}.pt')
            if os.path.exists(face_batch_path):
                # ä½¿ç”¨mmap_mode='r'å‡å°‘å†…å­˜ä½¿ç”¨
                face_batch = torch.load(face_batch_path, map_location='cpu', mmap_mode='r')
                batch_size = face_batch.size(0)
                self.batch_sizes.append(batch_size)
                self.batch_indices.extend([(batch_idx, i) for i in range(batch_size)])
                # é‡Šæ”¾å†…å­˜
                del face_batch
        
        print(f"åŠ è½½é¢„å¤„ç†æ•°æ®é›†: {self.total_sequences} ä¸ªæ ·æœ¬, {self.total_batches} ä¸ªæ‰¹æ¬¡")
    
    def __getitem__(self, index):
        import time
        start_time = time.time()
        
        # è·å–æ‰¹æ¬¡ç´¢å¼•å’Œæ ·æœ¬åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•
        batch_idx, sample_idx = self.batch_indices[index]
        
        # åŠ è½½æ‰¹æ¬¡æ•°æ®ï¼Œä½¿ç”¨mmap_modeå‡å°‘å†…å­˜ä½¿ç”¨
        face_data = torch.load(
            os.path.join(self.preprocessed_data_dir, 'face', f'face_batch_{batch_idx}.pt'),
            map_location='cpu',
            mmap_mode='r'
        )
        pose_data = torch.load(
            os.path.join(self.preprocessed_data_dir, 'pose', f'pose_batch_{batch_idx}.pt'),
            map_location='cpu',
            mmap_mode='r'
        )
        labels = torch.load(
            os.path.join(self.preprocessed_data_dir, 'labels', f'labels_batch_{batch_idx}.pt'),
            map_location='cpu',
            mmap_mode='r'
        )
        
        # è¿”å›å•ä¸ªæ ·æœ¬
        sample = (face_data[sample_idx], pose_data[sample_idx], labels[sample_idx])
        
        # é‡Šæ”¾å†…å­˜
        del face_data, pose_data, labels
        
        load_time = time.time() - start_time
        if load_time > 0.1:  # æ‰“å°åŠ è½½æ—¶é—´è¾ƒé•¿çš„æ ·æœ¬
            print(f"æ ·æœ¬ {index} åŠ è½½æ—¶é—´: {load_time:.4f}s")
        
        return sample
    
    def __len__(self):
        return self.total_sequences
    
    def get_all_labels(self):
        """è·å–æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾ï¼Œç”¨äºåˆ†å±‚é‡‡æ ·"""
        labels = []
        for batch_idx in range(self.total_batches):
            # åŠ è½½æ‰¹æ¬¡æ ‡ç­¾
            batch_labels = torch.load(
                os.path.join(self.preprocessed_data_dir, 'labels', f'labels_batch_{batch_idx}.pt'),
                map_location='cpu'
            )
            labels.extend(batch_labels.tolist())
        return labels

# --- 5. åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---  
print("æ­£åœ¨åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®é›†...")

# é…ç½®é¢„å¤„ç†æ•°æ®ç›®å½•
preprocessed_data_dir = r"/home/ccnu/Desktop/dataset/preprocessed_data"  # é¢„å¤„ç†åæ•°æ®ç›®å½•
# preprocessed_data_dir = r"D:\dataset\preprocessed_data"  # æœ¬åœ°æµ‹è¯•è·¯å¾„

# ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†
full_dataset = PreprocessedDataset(preprocessed_data_dir)

# è·å–æ‰€æœ‰æ ‡ç­¾ç”¨äºåˆ†å±‚é‡‡æ ·
all_labels = full_dataset.get_all_labels()
print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(pd.Series(all_labels).value_counts())}")

# è·å–ç´¢å¼•è¿›è¡Œåˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2,
    stratify=all_labels,  # æŒ‰ç±»åˆ«åˆ†å±‚é‡‡æ ·
    random_state=42
)

# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼ˆç›´æ¥ä½¿ç”¨Subsetï¼Œå› ä¸ºæ•°æ®å·²ç»é¢„å¤„ç†ï¼‰
train_dataset = torch.utils.data.Subset(full_dataset, train_idx)
val_dataset = torch.utils.data.Subset(full_dataset, val_idx)

# éªŒè¯è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç±»åˆ«åˆ†å¸ƒ
train_labels = [all_labels[i] for i in train_idx]
val_labels = [all_labels[i] for i in val_idx]

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {dict(pd.Series(train_labels).value_counts())}")
print(f"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {dict(pd.Series(val_labels).value_counts())}")

# è®¡ç®—ç±»åˆ«æ¯”ä¾‹ï¼Œç¡®ä¿åˆ†å¸ƒä¸€è‡´
train_label_ratio = {k: v/len(train_labels) for k, v in pd.Series(train_labels).value_counts().items()}
val_label_ratio = {k: v/len(val_labels) for k, v in pd.Series(val_labels).value_counts().items()}

print("è®­ç»ƒé›†ç±»åˆ«æ¯”ä¾‹:")
for k, v in sorted(train_label_ratio.items()):
    print(f"  ç±»åˆ« {k}: {v:.4f}")

print("éªŒè¯é›†ç±»åˆ«æ¯”ä¾‹:")
for k, v in sorted(val_label_ratio.items()):
    print(f"  ç±»åˆ« {k}: {v:.4f}")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

# --- 5. æ„å»ºèåˆæ¨¡å‹ --- 
class FusionResNetLSTM(nn.Module):
    def __init__(self, num_classes=5, sequence_length=5, hidden_size=512, num_layers=2):
        super(FusionResNetLSTM, self).__init__()
        
        # é¢éƒ¨åˆ†æ”¯
        self.face_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        # ä¿å­˜åŸå§‹ fc å±‚çš„ in_features
        self.feature_dim = self.face_backbone.fc.in_features
        self.face_backbone.fc = nn.Identity()
        
        # è‚¢ä½“åˆ†æ”¯
        self.pose_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.pose_backbone.fc = nn.Identity()
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.feature_dim * 2, 512),
            nn.ReLU(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # LSTM å±‚
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¾“å…¥åˆ° LSTM çš„ç‰¹å¾ç»´åº¦æ˜¯èåˆåçš„ç‰¹å¾ç»´åº¦
        self.lstm = nn.LSTM(
            input_size=self.feature_dim * 2,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.5
        )
        
        # èåˆåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(hidden_size * 2, num_classes)
        )
    
    def forward(self, face_x, pose_x):
        # è¾“å…¥ç»´åº¦: (batch_size, sequence_length, channels, height, width)
        batch_size = face_x.size(0)
        sequence_length = face_x.size(1)
        
        # è°ƒæ•´ç»´åº¦ä»¥é€‚åº” ResNet: (batch_size * sequence_length, channels, height, width)
        face_x_reshaped = face_x.view(-1, face_x.size(2), face_x.size(3), face_x.size(4))
        pose_x_reshaped = pose_x.view(-1, pose_x.size(2), pose_x.size(3), pose_x.size(4))
        
        # æå–ç‰¹å¾
        face_feat = self.face_backbone(face_x_reshaped)
        pose_feat = self.pose_backbone(pose_x_reshaped)
        
        # è°ƒæ•´ç‰¹å¾ç»´åº¦: (batch_size, sequence_length, feature_dim)
        face_feat = face_feat.view(batch_size, sequence_length, -1)
        pose_feat = pose_feat.view(batch_size, sequence_length, -1)
        
        # ç‰¹å¾èåˆä¸æ³¨æ„åŠ›åŠ æƒ
        fused_features = []
        for t in range(sequence_length):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾
            face_feat_t = face_feat[:, t, :]
            pose_feat_t = pose_feat[:, t, :]
            
            # ç‰¹å¾èåˆ
            combined = torch.cat([face_feat_t, pose_feat_t], dim=1)
            
            # æ³¨æ„åŠ›åŠ æƒ
            attention_weights = self.attention(combined)
            face_attn = attention_weights[:, 0].unsqueeze(1) * face_feat_t
            pose_attn = attention_weights[:, 1].unsqueeze(1) * pose_feat_t
            
            # åŠ æƒèåˆ
            fused = torch.cat([face_attn, pose_attn], dim=1)
            fused_features.append(fused.unsqueeze(1))
        
        # å †å æ‰€æœ‰æ—¶é—´æ­¥çš„èåˆç‰¹å¾: (batch_size, sequence_length, feature_dim * 2)
        fused_sequence = torch.cat(fused_features, dim=1)
        
        # LSTM å¤„ç†
        self.lstm.flatten_parameters()
        
        # å‰å‘ä¼ æ’­é€šè¿‡ LSTM
        lstm_out, _ = self.lstm(fused_sequence)
        
        # å¯¹äºåŒå‘ LSTMï¼Œå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰è¾“å‡ºï¼ˆåŒ…å«ä¸¤ä¸ªæ–¹å‘ï¼‰
        last_timestep_out = lstm_out[:, -1, :]
        output = self.classifier(last_timestep_out)
        
        return output

print(f"æ­£åœ¨åŠ è½½èåˆæ¨¡å‹å¹¶è¿è¡Œåœ¨: {device}")
model = FusionResNetLSTM(
    num_classes=num_classes,
    sequence_length=sequence_length,
    hidden_size=hidden_size,
    num_layers=num_layers
)
model = model.to(device)

# --- 6. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)

# --- 7. è®­ç»ƒå¾ªç¯ ---
# åˆå§‹åŒ–ç”¨äºè®°å½•ç»˜å›¾æ•°æ®çš„å­—å…¸
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

best_val_acc = 0.0
scaler = GradScaler()  # æ··åˆç²¾åº¦åŠ é€Ÿå™¨

print(f"å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {device}")

patience_counter = 0
early_stop_patience = 10

for epoch in range(num_epochs):
    import time
    epoch_start_time = time.time()
    
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0
    data_loading_time = 0
    training_time = 0

    for face_inputs, pose_inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        # è®°å½•æ•°æ®åŠ è½½æ—¶é—´
        batch_start_time = time.time()
        
        face_inputs, pose_inputs, labels = face_inputs.to(device), pose_inputs.to(device), labels.to(device)
        data_loading_time += time.time() - batch_start_time
        
        # è®°å½•è®­ç»ƒæ—¶é—´
        train_start_time = time.time()
        optimizer.zero_grad()

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(face_inputs, pose_inputs)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        training_time += time.time() - train_start_time

        # ç»Ÿè®¡
        running_loss += loss.item() * face_inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += face_inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0
    val_data_loading_time = 0
    val_inference_time = 0

    with torch.no_grad():
        for face_inputs, pose_inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            # è®°å½•éªŒè¯æ•°æ®åŠ è½½æ—¶é—´
            val_batch_start_time = time.time()
            face_inputs, pose_inputs, labels = face_inputs.to(device), pose_inputs.to(device), labels.to(device)
            val_data_loading_time += time.time() - val_batch_start_time
            
            # è®°å½•éªŒè¯æ¨ç†æ—¶é—´
            val_inference_start = time.time()
            with autocast(device_type='cuda'):
                outputs = model(face_inputs, pose_inputs)
                v_loss = criterion(outputs, labels)
            val_inference_time += time.time() - val_inference_start

            val_loss += v_loss.item() * face_inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += face_inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val
    scheduler.step(epoch_val_loss)
    
    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    epoch_end_time = time.time()
    epoch_duration = epoch_end_time - epoch_start_time
    
    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '  
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')
    print(f'æ€§èƒ½ç»Ÿè®¡: æ€»æ—¶é—´: {epoch_duration:.2f}s | æ•°æ®åŠ è½½: {data_loading_time:.2f}s | '  
          f'è®­ç»ƒ: {training_time:.2f}s | éªŒè¯æ•°æ®åŠ è½½: {val_data_loading_time:.2f}s | '  
          f'éªŒè¯æ¨ç†: {val_inference_time:.2f}s')
    print(f'æ•°æ®åŠ è½½å æ¯”: {(data_loading_time/epoch_duration*100):.1f}% | è®­ç»ƒå æ¯”: {(training_time/epoch_duration*100):.1f}%')

    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc
        patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

        # æ¸…é™¤æ—§çš„ best æ¨¡å‹ï¼ˆåªåˆ é™¤å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³çš„ï¼‰
        for old_file in glob.glob("best_model_acc_fusion_bilstm_*.pth"):
            # ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
            try:
                old_acc_str = old_file.split('_')[-1].split('.')[0]
                old_acc = int(old_acc_str) / 10000
                # åªæœ‰å½“æ—§æ¨¡å‹çš„å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³å‡†ç¡®ç‡æ—¶æ‰åˆ é™¤
                if old_acc < best_val_acc:
                    os.remove(old_file)
                    print(f"ğŸ”„ åˆ é™¤æ—§æ¨¡å‹: {old_file} (å‡†ç¡®ç‡: {old_acc:.4f})")
            except:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿåˆ é™¤
                os.remove(old_file)
                print(f"ğŸ”„ åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„æ—§æ¨¡å‹: {old_file}")

        # ä¿å­˜æ–°æ¨¡å‹
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_model_acc_fusion_bilstm_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")
    else:
        patience_counter += 1
        print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

        # è§¦å‘æ—©åœ
        if patience_counter >= early_stop_patience:
            print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
            break

# --- ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('fusion_bilstm_training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
