import glob
from torch import nn, optim, autocast
from torch.cuda.amp import GradScaler
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
from datetime import datetime, timedelta
from collections import defaultdict


class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_path, seq_len=20, transform=None):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform

        # 1. åŠ è½½æ ‡ç­¾
        self.label_df = pd.read_csv(csv_path)
        self.label_df['timestamp'] = pd.to_datetime(self.label_df['timestamp'])
        self.label_map = {'ä½': 1,
                          'ç¨ä½': 2,
                          'ä¸­æ€§': 3,
                          'ç¨é«˜': 4,
                          'é«˜': 5}

        # 2. è§£ææ–‡ä»¶å¹¶æŒ‰â€œæ®µâ€åˆ†ç»„
        # key: (start_time_str, end_time_str), value: list of file_info
        segments = defaultdict(list)
        all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]

        for f in all_files:
            parts = f.split('_')
            # å¸§åºå·: parts[1], å¼€å§‹æ—¶é—´: parts[4], ç»“æŸæ—¶é—´: parts[5]
            frame_idx = int(parts[1])
            s_time_str, e_time_str = parts[4], parts[5].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            curr_dt = start_dt + timedelta(seconds=frame_idx * 0.1)  # 10 FPS

            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. åœ¨æ¯ä¸ªæ®µå†…æ„å»ºè¿ç»­åºåˆ—
        self.valid_sequences = []
        for seg_key in segments:
            # ç¡®ä¿æ®µå†…æŒ‰å¸§åºå·æ’åº
            seg_files = sorted(segments[seg_key], key=lambda x: x['idx'])

            # åœ¨å½“å‰æ®µå†…æ»‘åŠ¨çª—å£
            for i in range(len(seg_files) - seq_len):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # åŒ¹é…æœ€æ¥è¿‘çš„æ ‡ç­¾ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰
                # å¯»æ‰¾æ ‡ç­¾æ—¶é—´ä¸å¸§æ—¶é—´è¯¯å·®åœ¨1ç§’ä»¥å†…çš„è®°å½•
                matched = self.label_df[
                    (self.label_df['timestamp'] >= end_frame_time - timedelta(seconds=1)) &
                    (self.label_df['timestamp'] <= end_frame_time + timedelta(seconds=1))
                    ]

                if not matched.empty:
                    label_str = matched.iloc[-1]['attention']
                    self.valid_sequences.append({
                        'files': [x['filename'] for x in seq_frames],
                        'label': self.label_map.get(label_str, 3)  # é»˜è®¤â€œä¸€èˆ¬â€
                    })
        print(f"Total sequences created: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            frames.append(img)
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long)


# ===========================
# 2. æ¨¡å‹å®šä¹‰ (ResNet50 + LSTM)
# ===========================
class ResNet50LSTM(nn.Module):
    def __init__(self, num_classes=5, hidden_size=512, num_lstm_layers=2):
        super(ResNet50LSTM, self).__init__()

        # åŠ è½½é¢„è®­ç»ƒçš„ ResNet50
        # ä½¿ç”¨æ–°çš„ weights API
        weights = models.ResNet50_Weights.IMAGENET1K_V1
        resnet = models.resnet50(weights=weights)

        # é‡è¦ï¼šResNet50 åœ¨å…¨è¿æ¥å±‚ä¹‹å‰çš„è¾“å‡ºç»´åº¦æ˜¯ 2048
        self.resnet_out_dim = resnet.fc.in_features  # 2048

        # å»æ‰ ResNet æœ€åçš„å…¨è¿æ¥åˆ†ç±»å±‚
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

        # å®šä¹‰ LSTM
        self.lstm = nn.LSTM(
            input_size=self.resnet_out_dim,  # è¾“å…¥ç»´åº¦å¿…é¡»æ˜¯ 2048
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=0.3  # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        # å®šä¹‰æœ€ç»ˆçš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        # è¾“å…¥ x å½¢çŠ¶: (Batch_Size, Seq_Len, C, H, W)
        b, s, c, h, w = x.shape

        # 1. CNN ç‰¹å¾æå–
        # å°† Batch å’Œ Seq ç»´åº¦åˆå¹¶ï¼Œä»¥ä¾¿å¹¶è¡Œå¤„ç†æ‰€æœ‰å›¾ç‰‡
        x_flat = x.view(b * s, c, h, w)

        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ä½¿ç”¨ torch.no_grad()ï¼Œå› ä¸º 4090 æ”¯æŒå…¨é‡å¾®è°ƒ
        features = self.feature_extractor(x_flat)
        # features å½¢çŠ¶: (B*S, 2048, 1, 1)

        # å±•å¹³å¹¶æ¢å¤æ—¶åºç»´åº¦
        features = features.view(b, s, -1)  # å½¢çŠ¶: (B, S, 2048)

        # 2. LSTM æ—¶åºå»ºæ¨¡
        self.lstm.flatten_parameters()  # ä¼˜åŒ–æ˜¾å­˜
        lstm_out, _ = self.lstm(features)
        # lstm_out å½¢çŠ¶: (B, S, hidden_size)

        # 3. åˆ†ç±»
        # æˆ‘ä»¬åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºæ•´ä¸ªåºåˆ—çš„é¢„æµ‹ç»“æœ
        last_timestep_out = lstm_out[:, -1, :]
        logits = self.classifier(last_timestep_out)

        return logits


# ===========================
# 3. é…ç½®ä¸è®­ç»ƒè„šæœ¬
# ===========================
if __name__ == '__main__':
    # --- é…ç½® ---
    IMG_DIR = r'E:\æ•°æ®\20231229 è®¡ç®—æœºç½‘ç»œè€ƒè¯•æ•°æ®æ±‡æ€»\ç¬¬1ç»„\è§†é¢‘\2021214387_å‘¨å©‰å©·\total\extracted_frames'  # <-- ä¿®æ”¹è¿™é‡Œ
    CSV_PATH = r'D:\GraduationProject\demo1\output\2021214387_å‘¨å©‰å©·.csv'  # <-- ä¿®æ”¹è¿™é‡Œ

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {DEVICE}")

    # é’ˆå¯¹ 4090 çš„è¶…å‚æ•°è®¾ç½®
    BATCH_SIZE = 24  # 24G æ˜¾å­˜å¯ä»¥å°è¯• 24 æˆ– 32
    SEQ_LEN = 30  # è¾“å…¥ 3 ç§’çš„è§†é¢‘ (10fps * 3s)
    NUM_EPOCHS = 15
    LEARNING_RATE = 3e-5  # å¾®è°ƒæ—¶å­¦ä¹ ç‡è¦å°
    NUM_CLASSES = 5

    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        # ImageNet æ ‡å‡†åŒ–
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # --- å®ä¾‹åŒ–æ•°æ®å’Œæ¨¡å‹ ---
    dataset = MultiSegmentAttentionDataset(img_dir=IMG_DIR, csv_path=CSV_PATH, seq_len=SEQ_LEN, transform=transform)

    # num_workers=8 åˆ©ç”¨å¤šæ ¸CPUåŠ é€Ÿæ•°æ®è¯»å–ï¼Œpin_memory=True åŠ é€Ÿæ•°æ®ä¼ å…¥GPU
    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)

    model = ResNet50LSTM(num_classes=NUM_CLASSES).to(DEVICE)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # å¦‚æœä½ çš„æ•°æ®ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼ˆä¾‹å¦‚â€œé«˜â€ç‰¹åˆ«å¤šï¼‰ï¼Œè€ƒè™‘ç»™ CrossEntropyLoss æ·»åŠ  weight å‚æ•°
    criterion = nn.CrossEntropyLoss()
    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼Œå¯¹å¾®è°ƒæ•ˆæœæ›´å¥½
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

    # æ··åˆç²¾åº¦è®­ç»ƒ GradScaler
    scaler = GradScaler()

    # --- è®­ç»ƒå‡†å¤‡ ---
    best_acc = 0.0  # åˆå§‹åŒ–æœ€é«˜å‡†ç¡®ç‡ä¸º0
    best_model_name = ""

    print("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(NUM_EPOCHS):
        model.train()
        running_loss = 0.0
        correct_preds = 0
        total_preds = 0

        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()

            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # ç»Ÿè®¡ä¿¡æ¯
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            total_preds += labels.size(0)
            correct_preds += (predicted == labels).sum().item()

            if (i + 1) % 10 == 0:
                print(f"[Epoch {epoch + 1}/{NUM_EPOCHS}, Step {i + 1}] Loss: {loss.item():.4f}")

        # è®¡ç®—è¯¥ Epoch çš„å¹³å‡æŒ‡æ ‡
        epoch_loss = running_loss / len(dataset)
        epoch_acc = correct_preds / total_preds
        print(f"--- Epoch {epoch + 1} Finished. Avg Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f} ---")

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä»…ä¿å­˜æœ€å¥½çš„æ¨¡å‹ ---
        if epoch_acc > best_acc:
            # 1. æ›´æ–°æœ€é«˜å‡†ç¡®ç‡è®°å½•
            best_acc = epoch_acc

            # 2. åˆ é™¤ä¹‹å‰ä¿å­˜è¿‡çš„ best_model æ–‡ä»¶ï¼ˆé˜²æ­¢ç¡¬ç›˜å †ç§¯ï¼‰
            # æœç´¢ç›®å½•ä¸‹æ‰€æœ‰ä»¥ 'best_model_acc_' å¼€å¤´çš„æ–‡ä»¶å¹¶åˆ é™¤
            for old_file in glob.glob("best_model_acc_*.pth"):
                try:
                    os.remove(old_file)
                except:
                    pass

                    # 3. æ„é€ æ–°çš„æ–‡ä»¶åå¹¶ä¿å­˜
            # ä¾‹å¦‚ï¼šbest_model_acc_9542.pth
            save_path = f'best_model_acc_{int(best_acc*10000)}.pth'
            torch.save(model.state_dict(), save_path)

            print(f"ğŸŒŸ æ£€æµ‹åˆ°æ›´å¥½çš„æ¨¡å‹ï¼å‡†ç¡®ç‡æé«˜åˆ°: {best_acc:.4f}ï¼Œå·²ä¿å­˜ä¸º {save_path}")
        else:
            print(f"â„¹ï¸ æœ¬è½®å‡†ç¡®ç‡ ({epoch_acc:.4f}) æœªè¶…è¿‡å†å²æœ€å¥½æˆç»© ({best_acc:.4f})ï¼Œä¸ä¿å­˜ã€‚")

    print(f"è®­ç»ƒç»“æŸ! æœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä¸º: {best_acc:.4f}")