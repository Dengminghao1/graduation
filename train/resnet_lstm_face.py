import glob

from matplotlib import pyplot as plt
from torch import nn, optim, autocast
from torch.cuda.amp import GradScaler
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from PIL import Image
from datetime import datetime, timedelta
from collections import defaultdict

from tqdm import tqdm


class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_path, seq_len=20, transform=None):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform

        # 1. åŠ è½½æ ‡ç­¾
        self.label_df = pd.read_csv(csv_path)
        self.label_df['timestamp'] = pd.to_datetime(self.label_df['timestamp'])
        self.label_map = {'ä½': 0,
                          'ç¨ä½': 1,
                          'ä¸­æ€§': 2,
                          'ç¨é«˜': 3,
                          'é«˜': 4}

        # 2. è§£ææ–‡ä»¶å¹¶æŒ‰â€œæ®µâ€åˆ†ç»„
        # key: (start_time_str, end_time_str), value: list of file_info
        segments = defaultdict(list)
        all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]

        for f in all_files:
            parts = f.split('_')
            # å¸§åºå·: parts[1], å¼€å§‹æ—¶é—´: parts[4], ç»“æŸæ—¶é—´: parts[5]
            frame_idx = int(parts[1])
            s_time_str, e_time_str = parts[4], parts[5].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            curr_dt = start_dt + timedelta(seconds=frame_idx * 0.1)  # 10 FPS

            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. åœ¨æ¯ä¸ªæ®µå†…æ„å»ºè¿ç»­åºåˆ—
        self.valid_sequences = []
        all_files = [f for f in os.listdir(self.img_dir) if f.endswith('.jpg')]

        for f in all_files:
            parts = f.split('_')
            frame_idx = int(parts[1])
            s_time_str, e_time_str = parts[4], parts[5].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            curr_dt = start_dt + timedelta(seconds=frame_idx * 0.1)

            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. æ¯æ®µå†…ï¼šä¸¥æ ¼æŒ‰â€œ1 ç§’ = 10 å¸§â€æ„å»ºæ ·æœ¬
        self.valid_sequences = []
        print("æ­£åœ¨æŒ‰ç§’åŒ¹é…æ ‡ç­¾å¹¶æ„å»ºåºåˆ—...")

        for seg_key in segments:
            # ç¡®ä¿æ®µå†…æŒ‰å¸§åºå·æ’åº
            seg_files = sorted(segments[seg_key], key=lambda x: x['idx'])

            # stride=10 æ„å‘³ç€æ¯ä¸€ç§’æå–ä¸€ä¸ªåºåˆ—
            # å¦‚æœæƒ³è®©æ•°æ®æ›´ä¸°å¯Œï¼Œå¯ä»¥å‡å° strideï¼›å¦‚æœæƒ³å‡å°‘å†—ä½™ï¼Œstride åº”ç­‰äº 10
            for i in range(0, len(seg_files) - seq_len, 10):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # --- ä¼˜åŒ–åŒ¹é…é€»è¾‘ï¼šå¯»æ‰¾ 1 ç§’å†…æœ€å‡†çš„é‚£ä¸€åˆ» ---
                time_diffs = (self.label_df['timestamp'] - end_frame_time).abs()
                closest_idx = time_diffs.idxmin()
                min_diff = time_diffs.min()

                if min_diff <= timedelta(seconds=1):
                    label_str = self.label_df.loc[closest_idx, 'attention']
                    self.valid_sequences.append({
                        'files': [x['filename'] for x in seq_frames],
                        'label': self.label_map.get(label_str, 2)
                    })
        print(f"æˆåŠŸåˆ›å»ºåºåˆ—æ€»æ•°: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            frames.append(img)
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long)


# ===========================
# 2. æ¨¡å‹å®šä¹‰ (ResNet50 + LSTM)
# ===========================
class ResNet50LSTM(nn.Module):
    def __init__(self, num_classes=5, hidden_size=512, num_lstm_layers=2):
        super(ResNet50LSTM, self).__init__()

        # åŠ è½½é¢„è®­ç»ƒçš„ ResNet50
        # ä½¿ç”¨æ–°çš„ weights API
        weights = models.ResNet50_Weights.IMAGENET1K_V1
        resnet = models.resnet50(weights=weights)

        # é‡è¦ï¼šResNet50 åœ¨å…¨è¿æ¥å±‚ä¹‹å‰çš„è¾“å‡ºç»´åº¦æ˜¯ 2048
        self.resnet_out_dim = resnet.fc.in_features  # 2048

        # å»æ‰ ResNet æœ€åçš„å…¨è¿æ¥åˆ†ç±»å±‚
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

        # å®šä¹‰ LSTM
        self.lstm = nn.LSTM(
            input_size=self.resnet_out_dim,  # è¾“å…¥ç»´åº¦å¿…é¡»æ˜¯ 2048
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=0.3  # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        # å®šä¹‰æœ€ç»ˆçš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        # è¾“å…¥ x å½¢çŠ¶: (Batch_Size, Seq_Len, C, H, W)
        b, s, c, h, w = x.shape

        # 1. CNN ç‰¹å¾æå–
        # å°† Batch å’Œ Seq ç»´åº¦åˆå¹¶ï¼Œä»¥ä¾¿å¹¶è¡Œå¤„ç†æ‰€æœ‰å›¾ç‰‡
        x_flat = x.view(b * s, c, h, w)

        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ä½¿ç”¨ torch.no_grad()ï¼Œå› ä¸º 4090 æ”¯æŒå…¨é‡å¾®è°ƒ
        features = self.feature_extractor(x_flat)
        # features å½¢çŠ¶: (B*S, 2048, 1, 1)

        # å±•å¹³å¹¶æ¢å¤æ—¶åºç»´åº¦
        features = features.view(b, s, -1)  # å½¢çŠ¶: (B, S, 2048)

        # 2. LSTM æ—¶åºå»ºæ¨¡
        self.lstm.flatten_parameters()  # ä¼˜åŒ–æ˜¾å­˜
        lstm_out, _ = self.lstm(features)
        # lstm_out å½¢çŠ¶: (B, S, hidden_size)

        # 3. åˆ†ç±»
        # æˆ‘ä»¬åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºæ•´ä¸ªåºåˆ—çš„é¢„æµ‹ç»“æœ
        last_timestep_out = lstm_out[:, -1, :]
        logits = self.classifier(last_timestep_out)

        return logits


# ===========================
# 3. é…ç½®ä¸è®­ç»ƒè„šæœ¬
# ===========================
if __name__ == '__main__':
    # --- é…ç½® ---
    # r"/home/ccnu/Desktop/2021214387_å‘¨å©‰å©·/total/classified_frames"
    IMG_DIR = r'E:\æ•°æ®\20231229 è®¡ç®—æœºç½‘ç»œè€ƒè¯•æ•°æ®æ±‡æ€»\ç¬¬1ç»„\è§†é¢‘\2021214387_å‘¨å©‰å©·\total\extracted_frames'  # <-- ä¿®æ”¹è¿™é‡Œ
    CSV_PATH = r'D:\GraduationProject\demo1\output\2021214387_å‘¨å©‰å©·.csv'  # <-- ä¿®æ”¹è¿™é‡Œ

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {DEVICE}")

    # é’ˆå¯¹ 4090 çš„è¶…å‚æ•°è®¾ç½®
    BATCH_SIZE = 24  # 24G æ˜¾å­˜å¯ä»¥å°è¯• 24 æˆ– 32
    SEQ_LEN = 10  # è¾“å…¥ 1 ç§’çš„è§†é¢‘ (10fps * 3s)
    NUM_EPOCHS = 15
    LEARNING_RATE = 3e-5  # å¾®è°ƒæ—¶å­¦ä¹ ç‡è¦å°
    NUM_CLASSES = 5

    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        # ImageNet æ ‡å‡†åŒ–
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # --- 1. æ•°æ®é›†åˆ‡åˆ†ä¸åŠ è½½ ---
    # å‡è®¾ä½ çš„ MultiSegmentAttentionDataset ç±»å·²ç»åœ¨ä¸Šæ–¹å®šä¹‰å¥½
    print("æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†...")
    full_dataset = MultiSegmentAttentionDataset(img_dir=IMG_DIR, csv_path=CSV_PATH, seq_len=SEQ_LEN,
                                                transform=transform)

    # æŒ‰ç…§ 8:2 åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    # æ³¨æ„ï¼šå¯¹äºè§†é¢‘ï¼Œæ›´å¥½çš„æ–¹å¼æ˜¯æŒ‰è§†é¢‘æ–‡ä»¶åˆ‡åˆ†ï¼Œè¿™é‡Œå…ˆä½¿ç”¨éšæœºåˆ‡åˆ†ç´¢å¼•
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)

    # --- 2. æ¨¡å‹åˆå§‹åŒ– ---
    model = ResNet50LSTM(num_classes=NUM_CLASSES).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scaler = GradScaler()

    # ç”¨äºç»˜å›¾çš„åˆ—è¡¨
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }

    best_val_acc = 0.0
    print(f"å¼€å§‹è®­ç»ƒ... è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")

    # --- 3. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(NUM_EPOCHS):
        # --- 1. è®­ç»ƒé˜¶æ®µ (Training Phase) ---
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0

        # ä½¿ç”¨ tqdm åŒ…è£… train_loader
        train_bar = tqdm(train_loader, desc=f"Epoch [{epoch + 1}/{NUM_EPOCHS}] Train")

        for inputs, labels in train_bar:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # ç»Ÿè®¡æ•°æ®
            current_batch_size = inputs.size(0)
            train_loss += loss.item() * current_batch_size
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡å³ä¾§çš„æ˜¾ç¤ºä¿¡æ¯ (å½“å‰ batch çš„ loss)
            train_bar.set_postfix(loss=f"{loss.item():.4f}")

        avg_train_loss = train_loss / len(train_dataset)
        avg_train_acc = train_correct / train_total

        # --- 2. éªŒè¯é˜¶æ®µ (Validation Phase) ---
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0

        # éªŒè¯é›†ä¹Ÿå»ºè®®åŠ ä¸Šè¿›åº¦æ¡ï¼Œå°¤å…¶å½“éªŒè¯é›†è¾ƒå¤§æ—¶
        val_bar = tqdm(val_loader, desc=f"Epoch [{epoch + 1}/{NUM_EPOCHS}] Val")

        with torch.no_grad():
            for inputs, labels in val_bar:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

                with autocast(device_type='cuda'):
                    outputs = model(inputs)
                    v_loss = criterion(outputs, labels)

                val_loss += v_loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                val_bar.set_postfix(val_loss=f"{v_loss.item():.4f}")

        avg_val_loss = val_loss / len(val_dataset)
        avg_val_acc = val_correct / val_total

        # --- 3. ç»“æœè®°å½•ä¸ä¿å­˜ ---
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)

        # æ‰“å°æœ€ç»ˆæ±‡æ€»ç»“æœ
        print(f"\nSummary - Epoch [{epoch + 1}/{NUM_EPOCHS}]: "
              f"Train Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.4f}")

        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            for old_file in glob.glob("best_model_acc_*.pth"):
                os.remove(old_file)

            acc_suffix = int(best_val_acc * 10000)
            save_path = f'best_model_acc_{acc_suffix}.pth'
            torch.save(model.state_dict(), save_path)
            print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")

    # --- 4. ç»˜åˆ¶ç»“æœå›¾åƒ ---
    plt.figure(figsize=(12, 5))

    # ç»˜åˆ¶ Loss æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # ç»˜åˆ¶ Accuracy æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Acc')
    plt.plot(history['val_acc'], label='Val Acc')
    plt.title('Training & Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_results.png')
    plt.show()

    print(f"è®­ç»ƒç»“æŸ! æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.4f}")