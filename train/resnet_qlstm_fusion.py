import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta
# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# --- 1. é…ç½®å‚æ•° ---
face_data_dir = r"/home/ccnu/Desktop/dataset/classified_frames_face_by_label_all"  # é¢éƒ¨æ•°æ®
pose_data_dir = r"/home/ccnu/Desktop/dataset/classified_frames_pose_by_label_all"  # è‚¢ä½“æ•°æ®
# æ·»åŠ ç½®ä¿¡åº¦CSVæ–‡ä»¶ç›®å½•å‚æ•°
CONF_CSV_PATH = r"/home/ccnu/Desktop/dataset/confidence.csv"  # ç½®ä¿¡åº¦æ•°æ®
batch_size = 32  # è¿›ä¸€æ­¥å‡å°ä»¥é€‚åº”åºåˆ—è¾“å…¥
num_epochs = 100
learning_rate = 0.0001
num_classes = 5  # ä½, ç¨ä½, ä¸­æ€§, ç¨é«˜, é«˜
sequence_length = 10  # å¸§åºåˆ—é•¿åº¦ï¼ˆçª—å£å¤§å°ä¸ºåï¼‰
hidden_size = 512  # LSTM éšè—å±‚å¤§å°
num_layers = 2  # LSTM å±‚æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Quality-aware Temporal Attention LSTM
class QTALSTMCell(nn.Module):
    """
    Quality-aware Temporal Attention LSTM Cell (Confidence-only Version)
    """

    def __init__(self, input_size, hidden_size, use_smooth=True):
        super(QTALSTMCell, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.use_smooth = use_smooth

        # LSTM gates
        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)

        # Learnable decay weight
        self.alpha = nn.Parameter(torch.tensor(1.0))

        # Adaptive decay strength
        self.lambda_param = nn.Parameter(torch.tensor(0.5))

        # For EMA smoothing
        self.register_buffer("conf_prev", torch.zeros(1))


    def forward(self, x_t, h_prev, c_prev, conf_t):
        """
        x_t     : [B, D]
        h_prev  : [B, H]
        c_prev  : [B, H]
        conf_t  : [B]   in [0,1]
        """

        # ---------- 1. Confidence smoothing (optional) ----------
        if self.use_smooth:
            if self.conf_prev.numel() != conf_t.numel():
                self.conf_prev = conf_t.detach()

            conf_s = 0.8 * self.conf_prev + 0.2 * conf_t
            self.conf_prev = conf_s.detach()
        else:
            conf_s = conf_t

        # ---------- 2. Quality-aware delta ----------
        alpha = torch.relu(self.alpha)

        delta_t = alpha * (1 - conf_s)

        # ---------- 3. Exponential decay ----------
        s_t = torch.exp(-delta_t).unsqueeze(1)

        # ---------- 4. Adaptive memory attenuation ----------
        lambda_ = torch.sigmoid(self.lambda_param)

        decay = 1 - lambda_ * (1 - s_t)

        c_prev = c_prev * decay

        # ---------- 5. LSTM gates ----------
        combined = torch.cat([x_t, h_prev], dim=1)

        gates = self.W(combined)

        f_t, i_t, o_t, g_t = gates.chunk(4, dim=1)

        f_t = torch.sigmoid(f_t)
        i_t = torch.sigmoid(i_t)
        o_t = torch.sigmoid(o_t)
        g_t = torch.tanh(g_t)

        # ---------- 6. Input modulation ----------
        i_t = i_t * s_t

        # ---------- 7. Cell update ----------
        c_t = f_t * c_prev + i_t * g_t

        h_t = o_t * torch.tanh(c_t)

        return h_t, c_t

class QTALSTM(nn.Module):

    def __init__(self, input_size, hidden_size, use_smooth=True):
        super(QTALSTM, self).__init__()

        self.hidden_size = hidden_size

        self.cell = QTALSTMCell(
            input_size,
            hidden_size,
            use_smooth
        )


    def forward(self, x, conf):
        """
        x    : [B, T, D]
        conf : [B, T]
        """

        B, T, _ = x.size()

        h = torch.zeros(B, self.hidden_size, device=x.device)
        c = torch.zeros(B, self.hidden_size, device=x.device)

        outputs = []

        for t in range(T):

            h, c = self.cell(
                x[:, t],
                h,
                c,
                conf[:, t]
            )

            outputs.append(h.unsqueeze(1))

        return torch.cat(outputs, dim=1)



# --- 2. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ---
# ResNet æ ‡å‡†è¾“å…¥æ˜¯ 224x224
# ä½¿ç”¨åŸæœ‰å‚æ•°ï¼šé¢éƒ¨å’Œè‚¢ä½“åˆ†åˆ«ä½¿ç”¨å„è‡ªçš„æ ‡å‡†åŒ–å‚æ•°
data_transforms = {
    'face_train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'face_val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'pose_train': transforms.Compose([
        transforms.Resize(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(5),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.03, 0.03),
            scale=(0.98, 1.02)
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
    'pose_val': transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
}

# --- 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» --- 
# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œæ”¯æŒæŒ‰æ—¶é—´åŒºé—´åˆ†ç»„é€‰æ‹©ä¸€å¸§
class TimeIntervalDataset(torch.utils.data.Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = []
        self.targets = []
        
        # éå†æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
        class_to_idx = {}
        for i, class_name in enumerate(sorted(os.listdir(data_dir))):
            class_to_idx[class_name] = i
            class_path = os.path.join(data_dir, class_name)
            if not os.path.isdir(class_path):
                continue
            
            # æŒ‰æ—¶é—´åŒºé—´åˆ†ç»„æ–‡ä»¶
            interval_groups = {}
            for filename in os.listdir(class_path):
                if filename.endswith('.jpg') or filename.endswith('.png'):
                    # æå–æ—¶é—´åŒºé—´ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg æˆ– 192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                    parts = filename.split('_')
                    if len(parts) >= 5:
                        # å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡ä»¶å
                        if filename.endswith('.jpg'):
                            # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                            interval = f"{parts[-2]}_{parts[-1].split('.')[0]}"
                        else:
                            # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                            # æ‰¾åˆ°æ—¶é—´åŒºé—´éƒ¨åˆ†ï¼ˆå€’æ•°ç¬¬4å’Œå€’æ•°ç¬¬3éƒ¨åˆ†ï¼‰
                            if len(parts) >= 6:
                                interval = f"{parts[-4]}_{parts[-3]}"
                            else:
                                continue
                        if interval not in interval_groups:
                            interval_groups[interval] = []
                        interval_groups[interval].append(filename)
            
            # ä¿ç•™æ¯ä¸ªæ—¶é—´åŒºé—´çš„æ‰€æœ‰å›¾ç‰‡
            for interval, files in interval_groups.items():
                if files:
                    # æŒ‰å¸§å·æ’åº
                    def get_frame_number(filename):
                        if filename.endswith('.jpg'):
                            # é¢éƒ¨æ ¼å¼ï¼šframe_000070_192.168.0.101_01_20231229164011_20231229165010.jpg
                            parts = filename.split('_')
                            if len(parts) >= 2 and parts[0] == 'frame':
                                try:
                                    return int(parts[1])
                                except:
                                    return 0
                        else:
                            # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                            parts = filename.split('_')
                            if len(parts) >= 2:
                                try:
                                    # æå–å€’æ•°ç¬¬äºŒéƒ¨åˆ†çš„æ•°å­—
                                    return int(parts[-2])
                                except:
                                    return 0
                        return 0
                    
                    files.sort(key=get_frame_number)
                    # ä¿ç•™æ‰€æœ‰å›¾ç‰‡
                    for file in files:
                        img_path = os.path.join(class_path, file)
                        self.samples.append(img_path)
                        self.targets.append(class_to_idx[class_name])
    
    def __len__(self):
        return len(self.samples)

# åº”ç”¨å˜æ¢çš„æ•°æ®é›†ç±»
class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, transform=None):
        self.dataset = dataset
        self.indices = indices
        self.transform = transform

    def __getitem__(self, index):
        img_path, target = self.dataset.samples[self.indices[index]], self.dataset.targets[self.indices[index]]
        from PIL import Image
        img = Image.open(img_path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, target

    def __len__(self):
        return len(self.indices)

# --- 4. è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å™¨ --- 
class FusionDataset(torch.utils.data.Dataset):
    def __init__(self, face_data_dir, pose_data_dir, conf_csv_path=None, sequence_length=5):
        self.sequence_length = sequence_length
        self.samples = []
        self.targets = []
        
        # åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®
        face_dataset = TimeIntervalDataset(face_data_dir)
        pose_dataset = TimeIntervalDataset(pose_data_dir)
        
        # åŠ è½½ç½®ä¿¡åº¦ CSV æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        self.conf_df = None
        if conf_csv_path:
            print(f"æ­£åœ¨åŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶: {conf_csv_path}...")
            try:
                # ç›´æ¥åŠ è½½å•ä¸ªç½®ä¿¡åº¦CSVæ–‡ä»¶
                self.conf_df = pd.read_csv(conf_csv_path)
                self.conf_df['timestamp'] = pd.to_datetime(self.conf_df['timestamp_pose'])
                print(f"æˆåŠŸåŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶ï¼Œå…± {len(self.conf_df)} æ¡è®°å½•")
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ„å»ºé¢éƒ¨æ ·æœ¬çš„æ˜ å°„ï¼š{æ—¶é—´åŒºé—´: {å¸§å·: (è·¯å¾„, æ ‡ç­¾)}}
        face_map = {}
        for img_path, target in zip(face_dataset.samples, face_dataset.targets):
            filename = os.path.basename(img_path)
            if filename.endswith('.jpg'):
                parts = filename.split('_')
                if len(parts) >= 5:
                    interval = f"{parts[-2]}_{parts[-1].split('.')[0]}"
                    frame_num = 0
                    if len(parts) >= 2 and parts[0] == 'frame':
                        try:
                            frame_num = int(parts[1])
                        except:
                            pass
                    if interval not in face_map:
                        face_map[interval] = {}
                    face_map[interval][frame_num] = (img_path, target)
        
        # æ„å»ºè‚¢ä½“æ ·æœ¬çš„æ˜ å°„ï¼š{æ—¶é—´åŒºé—´: {å¸§å·: è·¯å¾„}}
        pose_map = {}
        for img_path in pose_dataset.samples:
            filename = os.path.basename(img_path)
            if filename.endswith('.png'):
                parts = filename.split('_')
                if len(parts) >= 6:
                    interval = f"{parts[-4]}_{parts[-3]}"
                    frame_num = 0
                    try:
                        frame_num = int(parts[-2])
                    except:
                        pass
                    if interval not in pose_map:
                        pose_map[interval] = {}
                    pose_map[interval][frame_num] = img_path
        
        # åŒ¹é…é¢éƒ¨å’Œè‚¢ä½“æ ·æœ¬å¹¶ç”Ÿæˆåºåˆ—
        matched_count = 0
        for interval in face_map:
            if interval in pose_map:
                # è·å–è¯¥æ—¶é—´åŒºé—´å†…æ‰€æœ‰åŒ¹é…çš„å¸§å·
                common_frame_nums = sorted(list(set(face_map[interval].keys()) & set(pose_map[interval].keys())))
                
                # ç”Ÿæˆè¿ç»­çš„å¸§åºåˆ—ï¼Œçª—å£å¤§å°ä¸º sequence_lengthï¼Œæ­¥é•¿ä¸º sequence_length
                for i in range(0, len(common_frame_nums) - sequence_length + 1, sequence_length):
                    # ç›´æ¥å–è¿ç»­çš„çª—å£ï¼Œä¸éœ€è¦é¢å¤–æ£€æŸ¥
                    frame_sequence = common_frame_nums[i:i+sequence_length]
                    face_sequence = []
                    pose_sequence = []
                    conf_sequence = []
                    target = None
                    valid_sequence = True
                    
                    # æ”¶é›†åºåˆ—ä¸­çš„æ‰€æœ‰å¸§
                    for frame_num in frame_sequence:
                        face_path, target = face_map[interval][frame_num]
                        pose_path = pose_map[interval][frame_num]
                        face_sequence.append(face_path)
                        pose_sequence.append(pose_path)
                        
                        # ç”Ÿæˆå¸§çš„æ—¶é—´æˆ³ä»¥åŒ¹é…ç½®ä¿¡åº¦
                        if self.conf_df is not None:
                            # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´ä¿¡æ¯
                            face_filename = os.path.basename(face_path)
                            parts = face_filename.split('_')
                            if len(parts) >= 6:
                                # å‡è®¾æ—¶é—´æ ¼å¼ä¸º 20231229153000
                                time_str = parts[-2]
                                try:
                                    # è§£ææ—¶é—´
                                    frame_time = datetime.strptime(time_str, "%Y%m%d%H%M%S")
                                    # æ·»åŠ å¸§åç§»ï¼ˆå‡è®¾æ¯å¸§0.1ç§’ï¼‰
                                    frame_time += timedelta(seconds=frame_num * 0.1)
                                    
                                    # åœ¨ç½®ä¿¡åº¦ DataFrame ä¸­æ‰¾æœ€æ¥è¿‘çš„æ—¶é—´ç‚¹
                                    time_diffs = (self.conf_df['timestamp'] - frame_time).abs()
                                    closest_idx = time_diffs.idxmin()
                                    
                                    if time_diffs.min() <= timedelta(seconds=1):
                                        # å°è¯•è·å– confidence å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ä¸ºé»˜è®¤å€¼ 0.5
                                        confidence = self.conf_df.loc[closest_idx].get('confidence', 0.5)
                                        conf_sequence.append(confidence)
                                    else:
                                        # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„æ—¶é—´ç‚¹ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                                        conf_sequence.append(0.5)
                                except Exception as e:
                                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                                    conf_sequence.append(0.5)
                            else:
                                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                                conf_sequence.append(0.5)
                        else:
                            # å¦‚æœæ²¡æœ‰æä¾›ç½®ä¿¡åº¦æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                            conf_sequence.append(0.5)
                    
                    if target is not None and valid_sequence:
                        self.samples.append((face_sequence, pose_sequence, conf_sequence))
                        self.targets.append(target)
                        matched_count += 1
        
        print(f"æˆåŠŸåŒ¹é… {matched_count} å¯¹åºåˆ—æ ·æœ¬")
    
    def __getitem__(self, index):
        face_img_paths, pose_img_paths, conf_sequence = self.samples[index]
        target = self.targets[index]
        
        from PIL import Image
        face_imgs = []
        pose_imgs = []
        
        # åŠ è½½åºåˆ—ä¸­çš„æ‰€æœ‰å›¾åƒ
        for face_path, pose_path in zip(face_img_paths, pose_img_paths):
            face_img = Image.open(face_path).convert('RGB')
            pose_img = Image.open(pose_path).convert('RGB')
            face_imgs.append(face_img)
            pose_imgs.append(pose_img)
        
        return face_imgs, pose_imgs, conf_sequence, target
    
    def __len__(self):
        return len(self.samples)

# åº”ç”¨å˜æ¢çš„èåˆæ•°æ®é›†ç±»
class FusionApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, face_transform=None, pose_transform=None):
        self.dataset = dataset
        self.indices = indices
        self.face_transform = face_transform
        self.pose_transform = pose_transform

    def __getitem__(self, index):
        face_imgs, pose_imgs, conf_sequence, target = self.dataset[self.indices[index]]
        transformed_face_imgs = []
        transformed_pose_imgs = []
        
        # å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨å˜æ¢
        for face_img, pose_img in zip(face_imgs, pose_imgs):
            if self.face_transform:
                face_img = self.face_transform(face_img)
            if self.pose_transform:
                pose_img = self.pose_transform(pose_img)
            transformed_face_imgs.append(face_img)
            transformed_pose_imgs.append(pose_img)
        
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œç»´åº¦ä¸º (åºåˆ—é•¿åº¦, é€šé“, é«˜åº¦, å®½åº¦)
        transformed_face_imgs = torch.stack(transformed_face_imgs)
        transformed_pose_imgs = torch.stack(transformed_pose_imgs)
        
        # å°†ç½®ä¿¡åº¦åºåˆ—è½¬æ¢ä¸ºå¼ é‡
        conf_tensor = torch.tensor(conf_sequence, dtype=torch.float32)
        
        return transformed_face_imgs, transformed_pose_imgs, conf_tensor, target

    def __len__(self):
        return len(self.indices)

# --- 5. åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---  
print("æ­£åœ¨åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®é›†...")

# åˆ›å»ºå®Œæ•´çš„èåˆæ•°æ®é›†ï¼ˆå·²åŒ¹é…çš„æ ·æœ¬ï¼‰
full_dataset = FusionDataset(face_data_dir, pose_data_dir, CONF_CSV_PATH, sequence_length=sequence_length)

# è·å–ç´¢å¼•è¿›è¡Œåˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2,
    stratify=full_dataset.targets,  # ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    random_state=42
)

# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
train_dataset = FusionApplyTransform(
    full_dataset,
    train_idx,
    face_transform=data_transforms['face_train'],
    pose_transform=data_transforms['pose_train']
)
val_dataset = FusionApplyTransform(
    full_dataset,
    val_idx,
    face_transform=data_transforms['face_val'],
    pose_transform=data_transforms['pose_val']
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

# --- 5. æ„å»ºèåˆæ¨¡å‹ --- 
class FusionResNetLSTM(nn.Module):
    def __init__(self, num_classes=5, sequence_length=5, hidden_size=512, num_layers=2):
        super(FusionResNetLSTM, self).__init__()
        
        # é¢éƒ¨åˆ†æ”¯
        self.face_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        # ä¿å­˜åŸå§‹ fc å±‚çš„ in_features
        self.feature_dim = self.face_backbone.fc.in_features
        self.face_backbone.fc = nn.Identity()
        
        # è‚¢ä½“åˆ†æ”¯
        self.pose_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.pose_backbone.fc = nn.Identity()
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.feature_dim * 2, 512),
            nn.ReLU(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # QTALSTM å±‚
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¾“å…¥åˆ° QTALSTM çš„ç‰¹å¾ç»´åº¦æ˜¯èåˆåçš„ç‰¹å¾ç»´åº¦
        self.qlstm = QTALSTM(
            input_size=self.feature_dim * 2,
            hidden_size=hidden_size,
            use_smooth=True
        )
        
        # èåˆåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, face_x, pose_x, conf):
        # è¾“å…¥ç»´åº¦: (batch_size, sequence_length, channels, height, width)
        batch_size = face_x.size(0)
        sequence_length = face_x.size(1)
        
        # è°ƒæ•´ç»´åº¦ä»¥é€‚åº” ResNet: (batch_size * sequence_length, channels, height, width)
        face_x_reshaped = face_x.view(-1, face_x.size(2), face_x.size(3), face_x.size(4))
        pose_x_reshaped = pose_x.view(-1, pose_x.size(2), pose_x.size(3), pose_x.size(4))
        
        # æå–ç‰¹å¾
        face_feat = self.face_backbone(face_x_reshaped)
        pose_feat = self.pose_backbone(pose_x_reshaped)
        
        # è°ƒæ•´ç‰¹å¾ç»´åº¦: (batch_size, sequence_length, feature_dim)
        face_feat = face_feat.view(batch_size, sequence_length, -1)
        pose_feat = pose_feat.view(batch_size, sequence_length, -1)
        
        # ç‰¹å¾èåˆä¸æ³¨æ„åŠ›åŠ æƒ
        fused_features = []
        for t in range(sequence_length):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾
            face_feat_t = face_feat[:, t, :]
            pose_feat_t = pose_feat[:, t, :]
            
            # ç‰¹å¾èåˆ
            combined = torch.cat([face_feat_t, pose_feat_t], dim=1)
            
            # æ³¨æ„åŠ›åŠ æƒ
            attention_weights = self.attention(combined)
            face_attn = attention_weights[:, 0].unsqueeze(1) * face_feat_t
            pose_attn = attention_weights[:, 1].unsqueeze(1) * pose_feat_t
            
            # åŠ æƒèåˆ
            fused = torch.cat([face_attn, pose_attn], dim=1)
            fused_features.append(fused.unsqueeze(1))
        
        # å †å æ‰€æœ‰æ—¶é—´æ­¥çš„èåˆç‰¹å¾: (batch_size, sequence_length, feature_dim * 2)
        fused_sequence = torch.cat(fused_features, dim=1)
        
        # QTALSTM å¤„ç†ï¼Œä½¿ç”¨å¤–éƒ¨ä¼ é€’çš„ç½®ä¿¡åº¦
        qlstm_out = self.qlstm(fused_sequence, conf)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œåˆ†ç±»
        output = self.classifier(qlstm_out[:, -1, :])
        
        return output

print(f"æ­£åœ¨åŠ è½½èåˆæ¨¡å‹å¹¶è¿è¡Œåœ¨: {device}")
model = FusionResNetLSTM(
    num_classes=num_classes,
    sequence_length=sequence_length,
    hidden_size=hidden_size,
    num_layers=num_layers
)
model = model.to(device)

# --- 6. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)

# --- 7. è®­ç»ƒå¾ªç¯ ---
# åˆå§‹åŒ–ç”¨äºè®°å½•ç»˜å›¾æ•°æ®çš„å­—å…¸
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

best_val_acc = 0.0
scaler = GradScaler()  # æ··åˆç²¾åº¦åŠ é€Ÿå™¨

print(f"å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {device}")

patience_counter = 0
early_stop_patience = 10

for epoch in range(num_epochs):
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0

    for face_inputs, pose_inputs, conf, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        face_inputs, pose_inputs, conf, labels = face_inputs.to(device), pose_inputs.to(device), conf.to(device), labels.to(device)
        optimizer.zero_grad()

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(face_inputs, pose_inputs, conf)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡
        running_loss += loss.item() * face_inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += face_inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0

    with torch.no_grad():
        for face_inputs, pose_inputs, conf, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            face_inputs, pose_inputs, conf, labels = face_inputs.to(device), pose_inputs.to(device), conf.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(face_inputs, pose_inputs, conf)
                v_loss = criterion(outputs, labels)

            val_loss += v_loss.item() * face_inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += face_inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val
    scheduler.step(epoch_val_loss)
    
    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '  
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')

    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc
        patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

        # æ¸…é™¤æ—§çš„ best æ¨¡å‹ï¼ˆåªåˆ é™¤å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³çš„ï¼‰
        for old_file in glob.glob("best_model_acc_fusion_qlstm_*.pth"):
            # ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
            try:
                old_acc_str = old_file.split('_')[-1].split('.')[0]
                old_acc = int(old_acc_str) / 10000
                # åªæœ‰å½“æ—§æ¨¡å‹çš„å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³å‡†ç¡®ç‡æ—¶æ‰åˆ é™¤
                if old_acc < best_val_acc:
                    os.remove(old_file)
                    print(f"ğŸ”„ åˆ é™¤æ—§æ¨¡å‹: {old_file} (å‡†ç¡®ç‡: {old_acc:.4f})")
            except:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿåˆ é™¤
                os.remove(old_file)
                print(f"ğŸ”„ åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„æ—§æ¨¡å‹: {old_file}")

        # ä¿å­˜æ–°æ¨¡å‹
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_model_acc_fusion_qlstm_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")
    else:
        patience_counter += 1
        print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

        # è§¦å‘æ—©åœ
        if patience_counter >= early_stop_patience:
            print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
            break

# --- ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('fusion_qlstm_training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
