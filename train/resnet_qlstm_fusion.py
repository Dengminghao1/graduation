import glob
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset, Dataset
from sklearn.model_selection import train_test_split
import os
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt
# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# --- 1. é…ç½®å‚æ•° ---
face_data_dir = r"/home/ccnu/Desktop/dataset/extracted_frames_pic/face_extracted_frames_all"  # é¢éƒ¨æ•°æ®
pose_data_dir = r"/home/ccnu/Desktop/dataset/extracted_frames_pic/pose_extracted_frames_all"  # è‚¢ä½“æ•°æ®
csv_dir = r"/home/ccnu/Desktop/dataset/eeg_csv"  # æ ‡ç­¾CSVæ–‡ä»¶ç›®å½•

# face_data_dir = r"D:\dataset\frame_picture\face_extracted_frames_101"  # é¢éƒ¨æ•°æ®
# pose_data_dir = r"D:\dataset\frame_picture\pose_extracted_frames_101"  # è‚¢ä½“æ•°æ®
# csv_dir = r"D:\dataset\eeg_csv"  # æ ‡ç­¾CSVæ–‡ä»¶ç›®å½•

# æ·»åŠ ç½®ä¿¡åº¦CSVæ–‡ä»¶ç›®å½•å‚æ•°
CONF_CSV_PATH = r"/home/ccnu/Desktop/dataset/Dataset_align_face_pose_eeg_feature.csv"  # ç½®ä¿¡åº¦æ•°æ®
batch_size = 32  # è¿›ä¸€æ­¥å‡å°ä»¥é€‚åº”åºåˆ—è¾“å…¥
num_epochs = 100
learning_rate = 0.0001
num_classes = 5  # ä½, ç¨ä½, ä¸­æ€§, ç¨é«˜, é«˜
sequence_length = 10  # å¸§åºåˆ—é•¿åº¦ï¼ˆçª—å£å¤§å°ä¸ºåï¼‰
hidden_size = 512  # LSTM éšè—å±‚å¤§å°
num_layers = 2  # LSTM å±‚æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Quality-aware Temporal Attention LSTM
class QTALSTMCell(nn.Module):
    """
    Quality-aware Temporal Attention LSTM Cell (Confidence-only Version)
    """

    def __init__(self, input_size, hidden_size, use_smooth=True):
        super(QTALSTMCell, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.use_smooth = use_smooth

        # LSTM gates
        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)

        # Learnable decay weight
        self.alpha = nn.Parameter(torch.tensor(1.0))

        # Adaptive decay strength
        self.lambda_param = nn.Parameter(torch.tensor(0.5))

        # For EMA smoothing
        self.register_buffer("conf_prev", torch.zeros(1))


    def forward(self, x_t, h_prev, c_prev, conf_t):
        """
        x_t     : [B, D]
        h_prev  : [B, H]
        c_prev  : [B, H]
        conf_t  : [B]   in [0,1]
        """

        # ---------- 1. Confidence smoothing (optional) ----------
        if self.use_smooth:
            if self.conf_prev.numel() != conf_t.numel():
                self.conf_prev = conf_t.detach()

            conf_s = 0.8 * self.conf_prev + 0.2 * conf_t
            self.conf_prev = conf_s.detach()
        else:
            conf_s = conf_t

        # ---------- 2. Quality-aware delta ----------
        alpha = torch.relu(self.alpha)

        delta_t = alpha * (1 - conf_s)

        # ---------- 3. Exponential decay ----------
        s_t = torch.exp(-delta_t).unsqueeze(1)

        # ---------- 4. Adaptive memory attenuation ----------
        lambda_ = torch.sigmoid(self.lambda_param)

        decay = 1 - lambda_ * (1 - s_t)

        c_prev = c_prev * decay

        # ---------- 5. LSTM gates ----------
        combined = torch.cat([x_t, h_prev], dim=1)

        gates = self.W(combined)

        f_t, i_t, o_t, g_t = gates.chunk(4, dim=1)

        f_t = torch.sigmoid(f_t)
        i_t = torch.sigmoid(i_t)
        o_t = torch.sigmoid(o_t)
        g_t = torch.tanh(g_t)

        # ---------- 6. Input modulation ----------
        i_t = i_t * s_t

        # ---------- 7. Cell update ----------
        c_t = f_t * c_prev + i_t * g_t

        h_t = o_t * torch.tanh(c_t)

        return h_t, c_t

class QTALSTM(nn.Module):

    def __init__(self, input_size, hidden_size, use_smooth=True):
        super(QTALSTM, self).__init__()

        self.hidden_size = hidden_size

        self.cell = QTALSTMCell(
            input_size,
            hidden_size,
            use_smooth
        )


    def forward(self, x, conf):
        """
        x    : [B, T, D]
        conf : [B, T]
        """

        B, T, _ = x.size()

        h = torch.zeros(B, self.hidden_size, device=x.device)
        c = torch.zeros(B, self.hidden_size, device=x.device)

        outputs = []

        for t in range(T):

            h, c = self.cell(
                x[:, t],
                h,
                c,
                conf[:, t]
            )

            outputs.append(h.unsqueeze(1))

        return torch.cat(outputs, dim=1)



# --- MultiSegmentAttentionDataset ç±» --- 
class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_dir, seq_len=10, transform=None, segment_keys=None, is_pose=False):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform
        self.label_map = {'ä½': 0, 'ç¨ä½': 1, 'ä¸­æ€§': 2, 'ç¨é«˜': 3, 'é«˜': 4}
        self.is_pose = is_pose

        # 1. æ‰«æå¹¶åŠ è½½ CSV æ ‡ç­¾åº“
        # key: (start_time_str, end_time_str), value: DataFrame
        self.label_dfs = {}
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        print(f"æ­£åœ¨åŠ è½½ {len(csv_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶...")

        for cf in csv_files:
            # å‡è®¾ CSV æ–‡ä»¶åæ ¼å¼: xxxx_xxxx_20231229153000_20231229154000.csv
            parts = cf.replace('.csv', '').split('_')
            # æ ¹æ®ä½ çš„æ–‡ä»¶åè§„åˆ™ï¼Œå€’æ•°ç¬¬äºŒå’Œå€’æ•°ç¬¬ä¸€é€šå¸¸æ˜¯æ—¶é—´
            s_str, e_str = parts[-2], parts[-1]

            df = pd.read_csv(os.path.join(csv_dir, cf))
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            self.label_dfs[(s_str, e_str)] = df

        # 2. è§£æå›¾åƒæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ®µåˆ†ç»„
        segments = defaultdict(list)
        if self.is_pose:
            all_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]
        else:
            all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]
        print(f"æ­£åœ¨è§£æ {len(all_files)} ä¸ªå›¾åƒæ–‡ä»¶...")

        for f in all_files:
            parts = f.split('_')
            if self.is_pose:
                # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                if len(parts) >= 6:
                    frame_idx = int(parts[-2])
                    s_time_str = parts[-4]
                    e_time_str = parts[-3]
            else:
                # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                if len(parts) >= 5:
                    frame_idx = int(parts[1])
                    s_time_str = parts[-2]
                    e_time_str = parts[-1].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            total_milliseconds = frame_idx * 100  # 0.1ç§’ = 100æ¯«ç§’
            curr_dt = start_dt + timedelta(milliseconds=total_milliseconds)
            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. å¦‚æœæŒ‡å®šäº† segment_keysï¼Œåˆ™åªä¿ç•™è¿™äº›æ®µçš„æ•°æ®
        if segment_keys is not None:
            filtered_segments = {k: v for k, v in segments.items() if k in segment_keys}
        else:
            filtered_segments = segments

        # 4. æ„å»ºåºåˆ—å¹¶ä»"å¯¹åº”"çš„ DataFrame ä¸­å–æ ‡ç­¾
        self.valid_sequences = []
        print("å¼€å§‹æ—¶åºåŒ¹é…æ ‡ç­¾...")

        for seg_key, seg_files_list in filtered_segments.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ CSV æ ‡ç­¾
            if seg_key not in self.label_dfs:
                print(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°æ®µ {seg_key} å¯¹åº”çš„ CSV æ ‡ç­¾ï¼Œè·³è¿‡...")
                continue

            current_df = self.label_dfs[seg_key]
            seg_files = sorted(seg_files_list, key=lambda x: x['idx'])

            for i in range(0, len(seg_files) - seq_len + 1, 10):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # ä½¿ç”¨æœ€åä¸€å¸§çš„æ ‡ç­¾
                end_frame_time = seq_frames[-1]['time']
                time_diffs = (current_df['timestamp'] - end_frame_time).abs()
                closest_idx = time_diffs.idxmin()
                label_str = current_df.loc[closest_idx, 'attention']
                
                self.valid_sequences.append({
                    'files': [x['filename'] for x in seq_frames],
                    'label': self.label_map.get(label_str, 2)
                })

        print(f"æˆåŠŸæ„å»ºåºåˆ—æ€»æ•°: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            else:
                # å¦‚æœæ²¡æœ‰transformï¼Œè‡³å°‘è½¬æ¢ä¸ºtensor
                from torchvision import transforms
                img = transforms.ToTensor()(img)
            frames.append(img)
        # è¿”å›å¸§åºåˆ—å’Œæ ‡ç­¾
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long)


# --- 2. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ---
# ResNet æ ‡å‡†è¾“å…¥æ˜¯ 224x224
# ä½¿ç”¨åŸæœ‰å‚æ•°ï¼šé¢éƒ¨å’Œè‚¢ä½“åˆ†åˆ«ä½¿ç”¨å„è‡ªçš„æ ‡å‡†åŒ–å‚æ•°
data_transforms = {
    'face_train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'face_val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'pose_train': transforms.Compose([
        transforms.Resize(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(5),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.03, 0.03),
            scale=(0.98, 1.02)
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
    'pose_val': transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
}

# --- 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» --- 
# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œæ”¯æŒæŒ‰æ—¶é—´åŒºé—´åˆ†ç»„é€‰æ‹©ä¸€å¸§
class TimeIntervalDataset(torch.utils.data.Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = []
        self.targets = []
        
        # éå†æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
        class_to_idx = {}
        for i, class_name in enumerate(sorted(os.listdir(data_dir))):
            class_to_idx[class_name] = i
            class_path = os.path.join(data_dir, class_name)
            if not os.path.isdir(class_path):
                continue
            
            # æŒ‰æ—¶é—´åŒºé—´åˆ†ç»„æ–‡ä»¶
            interval_groups = {}
            for filename in os.listdir(class_path):
                if filename.endswith('.jpg') or filename.endswith('.png'):
                    # æå–æ—¶é—´åŒºé—´ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg æˆ– 192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                    parts = filename.split('_')
                    if len(parts) >= 5:
                        # å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡ä»¶å
                        if filename.endswith('.jpg'):
                            # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                            interval = f"{parts[-2]}_{parts[-1].split('.')[0]}"
                        else:
                            # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                            # æ‰¾åˆ°æ—¶é—´åŒºé—´éƒ¨åˆ†ï¼ˆå€’æ•°ç¬¬4å’Œå€’æ•°ç¬¬3éƒ¨åˆ†ï¼‰
                            if len(parts) >= 6:
                                interval = f"{parts[-4]}_{parts[-3]}"
                            else:
                                continue
                        if interval not in interval_groups:
                            interval_groups[interval] = []
                        interval_groups[interval].append(filename)
            
            # ä¿ç•™æ¯ä¸ªæ—¶é—´åŒºé—´çš„æ‰€æœ‰å›¾ç‰‡
            for interval, files in interval_groups.items():
                if files:
                    # æŒ‰å¸§å·æ’åº
                    def get_frame_number(filename):
                        if filename.endswith('.jpg'):
                            # é¢éƒ¨æ ¼å¼ï¼šframe_000070_192.168.0.101_01_20231229164011_20231229165010.jpg
                            parts = filename.split('_')
                            if len(parts) >= 2 and parts[0] == 'frame':
                                try:
                                    return int(parts[1])
                                except:
                                    return 0
                        else:
                            # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                            parts = filename.split('_')
                            if len(parts) >= 2:
                                try:
                                    # æå–å€’æ•°ç¬¬äºŒéƒ¨åˆ†çš„æ•°å­—
                                    return int(parts[-2])
                                except:
                                    return 0
                        return 0
                    
                    files.sort(key=get_frame_number)
                    # ä¿ç•™æ‰€æœ‰å›¾ç‰‡
                    for file in files:
                        img_path = os.path.join(class_path, file)
                        self.samples.append(img_path)
                        self.targets.append(class_to_idx[class_name])
    
    def __len__(self):
        return len(self.samples)

# åº”ç”¨å˜æ¢çš„æ•°æ®é›†ç±»
class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, transform=None):
        self.dataset = dataset
        self.indices = indices
        self.transform = transform

    def __getitem__(self, index):
        img_path, target = self.dataset.samples[self.indices[index]], self.dataset.targets[self.indices[index]]
        from PIL import Image
        img = Image.open(img_path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, target

    def __len__(self):
        return len(self.indices)

# --- 4. è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å™¨ --- 
class FusionDataset(torch.utils.data.Dataset):
    def __init__(self, face_data_dir, pose_data_dir, csv_dir, conf_csv_path=None, sequence_length=10):
        self.sequence_length = sequence_length
        self.samples = []
        self.targets = []
        
        # åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®
        face_dataset = MultiSegmentAttentionDataset(face_data_dir, csv_dir, sequence_length, is_pose=False)
        pose_dataset = MultiSegmentAttentionDataset(pose_data_dir, csv_dir, sequence_length, is_pose=True)
        
        # åŠ è½½ç½®ä¿¡åº¦ CSV æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        self.conf_df = None
        if conf_csv_path:
            print(f"æ­£åœ¨åŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶: {conf_csv_path}...")
            try:
                # ç›´æ¥åŠ è½½å•ä¸ªç½®ä¿¡åº¦CSVæ–‡ä»¶
                self.conf_df = pd.read_csv(conf_csv_path,usecols=["timestamp_pose", "confidence"])
                self.conf_df['timestamp'] = pd.to_datetime(self.conf_df['timestamp_pose'])
                print(f"æˆåŠŸåŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶ï¼Œå…± {len(self.conf_df)} æ¡è®°å½•")
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶å¤±è´¥: {e}")
        
        # æŒ‰åºåŒ¹é…é¢éƒ¨å’Œè‚¢ä½“æ ·æœ¬
        matched_count = 0
        
        # éå†é¢éƒ¨åºåˆ—
        for face_seq in face_dataset.valid_sequences:
            # è·å–é¢éƒ¨åºåˆ—çš„æ—¶é—´åŒºé—´å’Œå¸§å·
            first_face_img = face_seq['files'][0]
            face_parts = first_face_img.split('_')
            if len(face_parts) >= 5:
                # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                face_interval = f"{face_parts[-2]}_{face_parts[-1].split('.')[0]}"
                face_frame_num = int(face_parts[1])
                
                # åœ¨è‚¢ä½“åºåˆ—ä¸­æŸ¥æ‰¾åŒ¹é…çš„åºåˆ—
                for pose_seq in pose_dataset.valid_sequences:
                    first_pose_img = pose_seq['files'][0]
                    pose_parts = first_pose_img.split('_')
                    if len(pose_parts) >= 6:
                        # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                        pose_interval = f"{pose_parts[-4]}_{pose_parts[-3]}"
                        pose_frame_num = int(pose_parts[-2])
                        
                        # æ£€æŸ¥æ—¶é—´åŒºé—´å’Œå¸§å·æ˜¯å¦åŒ¹é…
                        if face_interval == pose_interval and abs(face_frame_num - pose_frame_num) == 0:
                            # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
                            if len(face_seq['files']) == sequence_length and len(pose_seq['files']) == sequence_length:
                                # ç”Ÿæˆç½®ä¿¡åº¦åºåˆ—
                                conf_sequence = []
                                
                                # ä» face_interval ä¸­æå–å¼€å§‹æ—¶é—´
                                parts = first_face_img.split('_')
                                if len(parts) >= 6:
                                    # face_interval çš„å¼€å§‹æ—¶é—´æ˜¯ parts[-2]
                                    start_time_str = face_parts[-2]
                                    
                                    try:
                                        # è§£æå¼€å§‹æ—¶é—´
                                        start_time = datetime.strptime(start_time_str, "%Y%m%d%H%M%S")
                                        
                                        # ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªå¸§ç”Ÿæˆç½®ä¿¡åº¦
                                        for i in range(sequence_length):
                                            # è®¡ç®—å½“å‰å¸§çš„å¸§å·
                                            frame_num = face_frame_num + i
                                            # æ·»åŠ å¸§åç§»ï¼ˆå‡è®¾æ¯å¸§0.1ç§’ï¼‰
                                            frame_time = start_time + timedelta(seconds=frame_num * 0.1)
                                            
                                            if self.conf_df is not None:
                                                # é¦–å…ˆå°è¯•é€šè¿‡æ—¶é—´ç›´æ¥åŒ¹é…æ‰¾åˆ° confidence
                                                try:
                                                    # æ‰¾åˆ°ä¸ frame_time å®Œå…¨åŒ¹é…çš„æ—¶é—´ç‚¹
                                                    matching_rows = self.conf_df[self.conf_df['timestamp'] == frame_time]
                                                    if not matching_rows.empty:
                                                        # å¦‚æœæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„æ—¶é—´ç‚¹ï¼Œä½¿ç”¨å¯¹åº”çš„ç½®ä¿¡åº¦
                                                        confidence = matching_rows.iloc[0]['confidence']
                                                    else:
                                                        # å¦‚æœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…çš„æ—¶é—´ç‚¹ï¼Œä½¿ç”¨æœ€æ¥è¿‘çš„åŒ¹é…
                                                        time_diffs = (self.conf_df['timestamp'] - frame_time).abs()
                                                        closest_idx = time_diffs.idxmin()
                                                        confidence = self.conf_df.loc[closest_idx, 'confidence']
                                                except (KeyError, IndexError):
                                                    # å¦‚æœå–ä¸åˆ°ä½¿ç”¨é»˜è®¤å€¼
                                                    confidence = 0.5
                                                conf_sequence.append(confidence)
                                            else:
                                                # å¦‚æœæ²¡æœ‰æä¾›ç½®ä¿¡åº¦æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                                                conf_sequence.append(0.5)
                                    except Exception as e:
                                        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                                        conf_sequence = [0.5] * sequence_length
                                else:
                                    # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                                    conf_sequence = [0.5] * sequence_length
                                
                                self.samples.append((face_seq['files'], pose_seq['files'], conf_sequence))
                                self.targets.append(face_seq['label'])
                                matched_count += 1
                                break
        
        print(f"æˆåŠŸåŒ¹é… {matched_count} å¯¹åºåˆ—æ ·æœ¬")
    
    def __getitem__(self, index):
        face_img_names, pose_img_names, conf_sequence = self.samples[index]
        target = self.targets[index]
        
        from PIL import Image
        face_imgs = []
        pose_imgs = []
        
        # åŠ è½½åºåˆ—ä¸­çš„æ‰€æœ‰å›¾åƒ
        for face_name, pose_name in zip(face_img_names, pose_img_names):
            face_img = Image.open(os.path.join(face_data_dir, face_name)).convert('RGB')
            pose_img = Image.open(os.path.join(pose_data_dir, pose_name)).convert('RGB')
            face_imgs.append(face_img)
            pose_imgs.append(pose_img)
        
        return face_imgs, pose_imgs, conf_sequence, target
    
    def __len__(self):
        return len(self.samples)

# åº”ç”¨å˜æ¢çš„èåˆæ•°æ®é›†ç±»
class FusionApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, face_transform=None, pose_transform=None):
        self.dataset = dataset
        self.indices = indices
        self.face_transform = face_transform
        self.pose_transform = pose_transform

    def __getitem__(self, index):
        face_imgs, pose_imgs, conf_sequence, target = self.dataset[self.indices[index]]
        transformed_face_imgs = []
        transformed_pose_imgs = []
        
        # å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨å˜æ¢
        for face_img, pose_img in zip(face_imgs, pose_imgs):
            if self.face_transform:
                face_img = self.face_transform(face_img)
            if self.pose_transform:
                pose_img = self.pose_transform(pose_img)
            transformed_face_imgs.append(face_img)
            transformed_pose_imgs.append(pose_img)
        
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œç»´åº¦ä¸º (åºåˆ—é•¿åº¦, é€šé“, é«˜åº¦, å®½åº¦)
        transformed_face_imgs = torch.stack(transformed_face_imgs)
        transformed_pose_imgs = torch.stack(transformed_pose_imgs)
        
        # å°†ç½®ä¿¡åº¦åºåˆ—è½¬æ¢ä¸ºå¼ é‡
        conf_tensor = torch.tensor(conf_sequence, dtype=torch.float32)
        
        return transformed_face_imgs, transformed_pose_imgs, conf_tensor, target

    def __len__(self):
        return len(self.indices)

# --- 5. åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---  
print("æ­£åœ¨åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®é›†...")

# åˆ›å»ºå®Œæ•´çš„èåˆæ•°æ®é›†ï¼ˆå·²åŒ¹é…çš„æ ·æœ¬ï¼‰
full_dataset = FusionDataset(face_data_dir, pose_data_dir, csv_dir, CONF_CSV_PATH, sequence_length=sequence_length)

# è·å–ç´¢å¼•è¿›è¡Œåˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2,
    stratify=full_dataset.targets,  # ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    random_state=42
)

# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
train_dataset = FusionApplyTransform(
    full_dataset,
    train_idx,
    face_transform=data_transforms['face_train'],
    pose_transform=data_transforms['pose_train']
)
val_dataset = FusionApplyTransform(
    full_dataset,
    val_idx,
    face_transform=data_transforms['face_val'],
    pose_transform=data_transforms['pose_val']
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

# --- 5. æ„å»ºèåˆæ¨¡å‹ --- 
class FusionResNetLSTM(nn.Module):
    def __init__(self, num_classes=5, sequence_length=5, hidden_size=512, num_layers=2):
        super(FusionResNetLSTM, self).__init__()
        
        # é¢éƒ¨åˆ†æ”¯
        self.face_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        # ä¿å­˜åŸå§‹ fc å±‚çš„ in_features
        self.feature_dim = self.face_backbone.fc.in_features
        self.face_backbone.fc = nn.Identity()
        
        # è‚¢ä½“åˆ†æ”¯
        self.pose_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.pose_backbone.fc = nn.Identity()
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.feature_dim * 2, 512),
            nn.ReLU(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # QTALSTM å±‚
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¾“å…¥åˆ° QTALSTM çš„ç‰¹å¾ç»´åº¦æ˜¯èåˆåçš„ç‰¹å¾ç»´åº¦
        self.qlstm = QTALSTM(
            input_size=self.feature_dim * 2,
            hidden_size=hidden_size,
            use_smooth=True
        )
        
        # èåˆåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, face_x, pose_x, conf):
        # è¾“å…¥ç»´åº¦: (batch_size, sequence_length, channels, height, width)
        batch_size = face_x.size(0)
        sequence_length = face_x.size(1)
        
        # è°ƒæ•´ç»´åº¦ä»¥é€‚åº” ResNet: (batch_size * sequence_length, channels, height, width)
        face_x_reshaped = face_x.view(-1, face_x.size(2), face_x.size(3), face_x.size(4))
        pose_x_reshaped = pose_x.view(-1, pose_x.size(2), pose_x.size(3), pose_x.size(4))
        
        # æå–ç‰¹å¾
        face_feat = self.face_backbone(face_x_reshaped)
        pose_feat = self.pose_backbone(pose_x_reshaped)
        
        # è°ƒæ•´ç‰¹å¾ç»´åº¦: (batch_size, sequence_length, feature_dim)
        face_feat = face_feat.view(batch_size, sequence_length, -1)
        pose_feat = pose_feat.view(batch_size, sequence_length, -1)
        
        # ç‰¹å¾èåˆä¸æ³¨æ„åŠ›åŠ æƒ
        fused_features = []
        for t in range(sequence_length):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾
            face_feat_t = face_feat[:, t, :]
            pose_feat_t = pose_feat[:, t, :]
            
            # ç‰¹å¾èåˆ
            combined = torch.cat([face_feat_t, pose_feat_t], dim=1)
            
            # æ³¨æ„åŠ›åŠ æƒ
            attention_weights = self.attention(combined)
            face_attn = attention_weights[:, 0].unsqueeze(1) * face_feat_t
            pose_attn = attention_weights[:, 1].unsqueeze(1) * pose_feat_t
            
            # åŠ æƒèåˆ
            fused = torch.cat([face_attn, pose_attn], dim=1)
            fused_features.append(fused.unsqueeze(1))
        
        # å †å æ‰€æœ‰æ—¶é—´æ­¥çš„èåˆç‰¹å¾: (batch_size, sequence_length, feature_dim * 2)
        fused_sequence = torch.cat(fused_features, dim=1)
        
        # QTALSTM å¤„ç†ï¼Œä½¿ç”¨å¤–éƒ¨ä¼ é€’çš„ç½®ä¿¡åº¦
        qlstm_out = self.qlstm(fused_sequence, conf)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œåˆ†ç±»
        output = self.classifier(qlstm_out[:, -1, :])
        
        return output

print(f"æ­£åœ¨åŠ è½½èåˆæ¨¡å‹å¹¶è¿è¡Œåœ¨: {device}")
model = FusionResNetLSTM(
    num_classes=num_classes,
    sequence_length=sequence_length,
    hidden_size=hidden_size,
    num_layers=num_layers
)
model = model.to(device)

# --- 6. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)

# --- 7. è®­ç»ƒå¾ªç¯ ---
# åˆå§‹åŒ–ç”¨äºè®°å½•ç»˜å›¾æ•°æ®çš„å­—å…¸
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

best_val_acc = 0.0
scaler = GradScaler()  # æ··åˆç²¾åº¦åŠ é€Ÿå™¨

print(f"å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {device}")

patience_counter = 0
early_stop_patience = 10

for epoch in range(num_epochs):
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0

    for face_inputs, pose_inputs, conf, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        face_inputs, pose_inputs, conf, labels = face_inputs.to(device), pose_inputs.to(device), conf.to(device), labels.to(device)
        optimizer.zero_grad()

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(face_inputs, pose_inputs, conf)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡
        running_loss += loss.item() * face_inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += face_inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0

    with torch.no_grad():
        for face_inputs, pose_inputs, conf, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            face_inputs, pose_inputs, conf, labels = face_inputs.to(device), pose_inputs.to(device), conf.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(face_inputs, pose_inputs, conf)
                v_loss = criterion(outputs, labels)

            val_loss += v_loss.item() * face_inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += face_inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val
    scheduler.step(epoch_val_loss)
    
    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '  
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')

    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc
        patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

        # æ¸…é™¤æ—§çš„ best æ¨¡å‹ï¼ˆåªåˆ é™¤å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³çš„ï¼‰
        for old_file in glob.glob("best_model_acc_fusion_qlstm_*.pth"):
            # ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
            try:
                old_acc_str = old_file.split('_')[-1].split('.')[0]
                old_acc = int(old_acc_str) / 10000
                # åªæœ‰å½“æ—§æ¨¡å‹çš„å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³å‡†ç¡®ç‡æ—¶æ‰åˆ é™¤
                if old_acc < best_val_acc:
                    os.remove(old_file)
                    print(f"ğŸ”„ åˆ é™¤æ—§æ¨¡å‹: {old_file} (å‡†ç¡®ç‡: {old_acc:.4f})")
            except:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿåˆ é™¤
                os.remove(old_file)
                print(f"ğŸ”„ åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„æ—§æ¨¡å‹: {old_file}")

        # ä¿å­˜æ–°æ¨¡å‹
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_model_acc_fusion_qlstm_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")
    else:
        patience_counter += 1
        print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

        # è§¦å‘æ—©åœ
        if patience_counter >= early_stop_patience:
            print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
            break

# --- ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('fusion_qlstm_training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
