import glob
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from torch import nn, optim, autocast
from torch.cuda.amp import GradScaler
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from PIL import Image
from datetime import datetime, timedelta
from collections import defaultdict
from tqdm import tqdm

# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_dir, seq_len=20, transform=None, segment_keys=None):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform
        self.label_map = {'ä½': 0, 'ç¨ä½': 1, 'ä¸­æ€§': 2, 'ç¨é«˜': 3, 'é«˜': 4}

        # 1. æ‰«æå¹¶åŠ è½½ CSV æ ‡ç­¾åº“
        # key: (start_time_str, end_time_str), value: DataFrame
        self.label_dfs = {}
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        print(f"æ­£åœ¨åŠ è½½ {len(csv_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶...")

        for cf in csv_files:
            # å‡è®¾ CSV æ–‡ä»¶åæ ¼å¼: xxxx_xxxx_20231229153000_20231229154000.csv
            parts = cf.replace('.csv', '').split('_')
            # æ ¹æ®ä½ çš„æ–‡ä»¶åè§„åˆ™ï¼Œå€’æ•°ç¬¬äºŒå’Œå€’æ•°ç¬¬ä¸€é€šå¸¸æ˜¯æ—¶é—´
            s_str, e_str = parts[-2], parts[-1]

            df = pd.read_csv(os.path.join(csv_dir, cf))
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            self.label_dfs[(s_str, e_str)] = df

        # 2. è§£æå›¾åƒæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ®µåˆ†ç»„
        segments = defaultdict(list)
        all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]
        for f in all_files:
            parts = f.split('_')
            # å‡è®¾å›¾åƒæ–‡ä»¶å: xxxx_frameidx_xxxx_xxxx_å¼€å§‹æ—¶é—´_ç»“æŸæ—¶é—´.jpg
            # è¯·æ ¹æ®å®é™…æƒ…å†µç¡®è®¤ parts çš„ç´¢å¼•
            frame_idx = int(parts[1])
            s_time_str = parts[4]
            e_time_str = parts[5].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            curr_dt = start_dt + timedelta(seconds=frame_idx * 0.1)

            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. å¦‚æœæŒ‡å®šäº† segment_keysï¼Œåˆ™åªä¿ç•™è¿™äº›æ®µçš„æ•°æ®
        if segment_keys is not None:
            filtered_segments = {k: v for k, v in segments.items() if k in segment_keys}
        else:
            filtered_segments = segments

        # 4. æ„å»ºåºåˆ—å¹¶ä»â€œå¯¹åº”â€çš„ DataFrame ä¸­å–æ ‡ç­¾
        self.valid_sequences = []
        print("å¼€å§‹æ—¶åºåŒ¹é…æ ‡ç­¾...")

        for seg_key, seg_files_list in filtered_segments.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ CSV æ ‡ç­¾
            if seg_key not in self.label_dfs:
                print(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°æ®µ {seg_key} å¯¹åº”çš„ CSV æ ‡ç­¾ï¼Œè·³è¿‡...")
                continue

            current_df = self.label_dfs[seg_key]
            seg_files = sorted(seg_files_list, key=lambda x: x['idx'])

            for i in range(0, len(seg_files) - seq_len,10):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # åœ¨å½“å‰æ®µæ‰€å±çš„ DataFrame ä¸­æ‰¾æœ€æ¥è¿‘çš„æ—¶é—´ç‚¹
                time_diffs = (current_df['timestamp'] - end_frame_time).abs()
                closest_idx = time_diffs.idxmin()

                if time_diffs.min() <= timedelta(seconds=1):
                    label_str = current_df.loc[closest_idx, 'attention']
                    self.valid_sequences.append({
                        'files': [x['filename'] for x in seq_frames],
                        'label': self.label_map.get(label_str, 2)
                    })

        print(f"æˆåŠŸæ„å»ºåºåˆ—æ€»æ•°: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            frames.append(img)
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long)


# ===========================
# 2. æ¨¡å‹å®šä¹‰ (ResNet50 + LSTM)
# ===========================
class ResNet50LSTM(nn.Module):
    def __init__(self, num_classes=5, hidden_size=512, num_lstm_layers=2):
        super(ResNet50LSTM, self).__init__()
        weights = models.ResNet50_Weights.IMAGENET1K_V1
        resnet = models.resnet50(weights=weights)

        self.resnet_out_dim = resnet.fc.in_features
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

        # æ–°å¢ï¼šç‰¹å¾æ ‡å‡†åŒ–å±‚ï¼Œé˜²æ­¢ CNN è¾“å‡ºèŒƒå›´æ³¢åŠ¨è¿‡å¤§
        self.bn = nn.BatchNorm1d(self.resnet_out_dim)

        self.lstm = nn.LSTM(
            input_size=self.resnet_out_dim,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            bidirectional=True,  # æ”¹ä¸ºåŒå‘ LSTM
            dropout=0.5  # å¢åŠ  Dropout
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, 256),  # åŒå‘ LSTM çš„è¾“å‡ºç»´åº¦ç¿»å€
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        b, s, c, h, w = x.shape
        x_flat = x.view(b * s, c, h, w)
        features = self.feature_extractor(x_flat)  # (B*S, 2048, 1, 1)

        # å±•å¹³å¹¶æ ‡å‡†åŒ–
        features = features.view(b * s, -1)
        features = self.bn(features)

        # æ¢å¤æ—¶åºç»´åº¦
        features = features.view(b, s, -1)

        self.lstm.flatten_parameters()
        lstm_out, _ = self.lstm(features)

        # å¯¹äºåŒå‘ LSTMï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶è€ƒè™‘ä¸¤ä¸ªæ–¹å‘çš„è¾“å‡º
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ­£å‘è¾“å‡ºå’Œç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„åå‘è¾“å‡º
        # æˆ–è€…æ›´ç®€å•çš„æ–¹æ³•ï¼šå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰è¾“å‡ºï¼ˆåŒ…å«ä¸¤ä¸ªæ–¹å‘ï¼‰
        last_timestep_out = lstm_out[:, -1, :]
        return self.classifier(last_timestep_out)


# ===========================
# 3. é…ç½®ä¸è®­ç»ƒè„šæœ¬
# ===========================
if __name__ == '__main__':
    # --- é…ç½® ---
    IMG_DIR = r'/home/ccnu/Desktop/dataset/extracted_frames_pic/face_extracted_frames_all'  # <-- ä¿®æ”¹è¿™é‡Œ
    CSV_DIR = r'/home/ccnu/Desktop/dataset/eeg_csv'  # <-- ä¿®æ”¹è¿™é‡Œ
    # IMG_DIR = r'/home/ccnu/Desktop/dataset/frames_face_all'  # <-- ä¿®æ”¹è¿™é‡Œ
    # CSV_DIR = r'/home/ccnu/Desktop/dataset/eeg_csv'  # <-- ä¿®æ”¹è¿™é‡Œ

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {DEVICE}")

    # é’ˆå¯¹ 4090 çš„è¶…å‚æ•°è®¾ç½®
    BATCH_SIZE = 32  # å¢åŠ æ‰¹é‡å¤§å°ä»¥æé«˜æ³›åŒ–èƒ½åŠ›
    SEQ_LEN = 10  # è¾“å…¥ 1 ç§’çš„è§†é¢‘ (10fps * 3s)
    NUM_EPOCHS = 50  # å¢åŠ è®­ç»ƒè½®æ•°
    LEARNING_RATE = 1e-5  # æ›´å°çš„åˆå§‹å­¦ä¹ ç‡
    NUM_CLASSES = 5

    # å›¾åƒé¢„å¤„ç†
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize(224),
            # transforms.RandomHorizontalFlip(),
            # transforms.ColorJitter(0.2, 0.2, 0.2),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    # --- 1. æ•°æ®é›†åˆ‡åˆ†ä¸åŠ è½½ ---
    # å‡è®¾ä½ çš„ MultiSegmentAttentionDataset ç±»å·²ç»åœ¨ä¸Šæ–¹å®šä¹‰å¥½
    print("æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†...")
    # 1. é¦–å…ˆè§£æå‡ºæ‰€æœ‰çš„æ®µ key
    all_files = [f for f in os.listdir(IMG_DIR) if f.endswith('.jpg')]
    temp_segments = set()
    for f in all_files:
        parts = f.split('_')
        temp_segments.add((parts[4], parts[5].replace('.jpg', '')))
    all_keys = list(temp_segments)

    # # 2. æŒ‰â€œæ®µâ€åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯ï¼ˆç¡®ä¿éªŒè¯é›†æ˜¯å…¨æ–°çš„è§†é¢‘æ®µï¼‰
    # train_keys, val_keys = train_test_split(all_keys, test_size=0.2, random_state=42)
    #
    # # 3. å®ä¾‹åŒ–ä¸¤ä¸ªç‹¬ç«‹çš„ Dataset
    # train_dataset = MultiSegmentAttentionDataset(IMG_DIR, CSV_DIR, SEQ_LEN, data_transforms['train'], segment_keys=train_keys)
    # val_dataset = MultiSegmentAttentionDataset(IMG_DIR, CSV_DIR, SEQ_LEN, data_transforms['val'], segment_keys=val_keys)
    #
    # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    # val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    full_dataset = MultiSegmentAttentionDataset(IMG_DIR, CSV_DIR, SEQ_LEN, data_transforms['train'])

    # è·å–æ•°æ®é›†é•¿åº¦å¹¶è¿›è¡Œéšæœºåˆ‡åˆ†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size

    # ä½¿ç”¨random_splitè¿›è¡Œéšæœºåˆ‡åˆ†
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    # --- 2. æ¨¡å‹åˆå§‹åŒ– ---
    model = ResNet50LSTM(num_classes=NUM_CLASSES).to(DEVICE)
    # ä¸resnet_face.pyä¿æŒä¸€è‡´
    criterion = nn.CrossEntropyLoss()
    # --- åœ¨åˆå§‹åŒ–ä¼˜åŒ–å™¨åæ·»åŠ  ---
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)  # ä¸resnet_face.pyä¿æŒä¸€è‡´
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)  # ä¸resnet_face.pyä¿æŒä¸€è‡´
    scaler = GradScaler()

    # ç”¨äºç»˜å›¾çš„åˆ—è¡¨
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }

    best_val_acc = 0.0
    print(f"å¼€å§‹è®­ç»ƒ... è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")

    patience_counter = 0
    early_stop_patience = 10
    # --- 3. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(NUM_EPOCHS):
        # --- 1. è®­ç»ƒé˜¶æ®µ (Training Phase) ---
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0

        # ä½¿ç”¨ tqdm åŒ…è£… train_loader
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]")

        for inputs, labels in train_bar:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # ç»Ÿè®¡æ•°æ®
            current_batch_size = inputs.size(0)
            train_loss += loss.item() * current_batch_size
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡å³ä¾§çš„æ˜¾ç¤ºä¿¡æ¯ (å½“å‰ batch çš„ loss)
            train_bar.set_postfix(loss=f"{loss.item():.4f}")

        avg_train_loss = train_loss / len(train_dataset)
        avg_train_acc = train_correct / train_total

        # --- 2. éªŒè¯é˜¶æ®µ (Validation Phase) ---
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0

        # éªŒè¯é›†ä¹Ÿå»ºè®®åŠ ä¸Šè¿›åº¦æ¡ï¼Œå°¤å…¶å½“éªŒè¯é›†è¾ƒå¤§æ—¶
        val_bar = tqdm(val_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Val]")

        with torch.no_grad():
            for inputs, labels in val_bar:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

                with autocast(device_type='cuda'):
                    outputs = model(inputs)
                    v_loss = criterion(outputs, labels)

                val_loss += v_loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                val_bar.set_postfix(val_loss=f"{v_loss.item():.4f}")

        avg_val_loss = val_loss / len(val_dataset)
        avg_val_acc = val_correct / val_total
        scheduler.step(avg_val_loss)  # è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
        # åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œåœ¨ scheduler.step() åæ·»åŠ 
        if epoch % 1 == 0:  # æ¯éš”ä¸€å®šepochè¾“å‡ºä¸€æ¬¡
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Learning Rate: {current_lr:.2e}")
        # --- 3. ç»“æœè®°å½•ä¸ä¿å­˜ ---
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)

        # æ‰“å°æœ€ç»ˆæ±‡æ€»ç»“æœ
        print(f"\nSummary - Epoch [{epoch + 1}/{NUM_EPOCHS}]: "
              f"Train Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.4f}")

        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
# æ¸…é™¤æ—§çš„ best æ¨¡å‹ï¼ˆåªåˆ é™¤å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³çš„ï¼‰
        for old_file in glob.glob("best_model_acc_face_lstm_*.pth"):
            # ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
            try:
                old_acc_str = old_file.split('_')[-1].split('.')[0]
                old_acc = int(old_acc_str) / 10000
                # åªæœ‰å½“æ—§æ¨¡å‹çš„å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³å‡†ç¡®ç‡æ—¶æ‰åˆ é™¤
                if old_acc < best_val_acc:
                    os.remove(old_file)
                    print(f"ğŸ”„ åˆ é™¤æ—§æ¨¡å‹: {old_file} (å‡†ç¡®ç‡: {old_acc:.4f})")
            except:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿåˆ é™¤
                os.remove(old_file)
                print(f"ğŸ”„ åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„æ—§æ¨¡å‹: {old_file}")

            acc_suffix = int(best_val_acc * 10000)
            save_path = f'best_model_acc_{acc_suffix}.pth'
            torch.save(model.state_dict(), save_path)
            print(f"ïŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")
        else:
            patience_counter += 1
            print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

            # è§¦å‘æ—©åœ
        if patience_counter >= early_stop_patience:
            print("ï›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
            break

    # --- 4. ç»˜åˆ¶ç»“æœå›¾åƒ ---
    plt.figure(figsize=(12, 5))

    # ç»˜åˆ¶ Loss æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # ç»˜åˆ¶ Accuracy æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Acc')
    plt.plot(history['val_acc'], label='Val Acc')
    plt.title('Training & Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_results_lstm.png')
    plt.show()

    print(f"è®­ç»ƒç»“æŸ! æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.4f}")