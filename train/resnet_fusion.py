import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm
import matplotlib.pyplot as plt

# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# --- 1. é…ç½®å‚æ•° ---
face_data_dir = r"/home/ccnu/Desktop/dataset/classified_frames_face_by_label_all"  # é¢éƒ¨æ•°æ®
pose_data_dir = r"/home/ccnu/Desktop/dataset/classified_frames_pose_by_label_all"  # è‚¢ä½“æ•°æ®
batch_size = 128  # å‡åŠä»¥é€‚åº”åŒè¾“å…¥
num_epochs = 100
learning_rate = 0.0001
num_classes = 5  # ä½, ç¨ä½, ä¸­æ€§, ç¨é«˜, é«˜
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ---
# ResNet æ ‡å‡†è¾“å…¥æ˜¯ 224x224
# ä½¿ç”¨åŸæœ‰å‚æ•°ï¼šé¢éƒ¨å’Œè‚¢ä½“åˆ†åˆ«ä½¿ç”¨å„è‡ªçš„æ ‡å‡†åŒ–å‚æ•°
data_transforms = {
    'face_train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'face_val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'pose_train': transforms.Compose([
        transforms.Resize(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(5),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.03, 0.03),
            scale=(0.98, 1.02)
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
    'pose_val': transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
}

# --- 3. è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å™¨ --- 
class FusionDataset(torch.utils.data.Dataset):
    def __init__(self, face_subset, pose_subset, face_transform=None, pose_transform=None):
        self.face_subset = face_subset
        self.pose_subset = pose_subset
        self.face_transform = face_transform
        self.pose_transform = pose_transform
    
    def __getitem__(self, index):
        # è·å–é¢éƒ¨å›¾åƒå’Œæ ‡ç­¾
        face_img, label = self.face_subset[index]
        # è·å–å¯¹åº”ç´¢å¼•çš„è‚¢ä½“å›¾åƒ
        pose_img, _ = self.pose_subset[index]
        
        if self.face_transform:
            face_img = self.face_transform(face_img)
        if self.pose_transform:
            pose_img = self.pose_transform(pose_img)
        
        return face_img, pose_img, label
    
    def __len__(self):
        return len(self.face_subset)

# --- 4. åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---
print("æ­£åœ¨åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®é›†...")
face_full_dataset = datasets.ImageFolder(face_data_dir)
pose_full_dataset = datasets.ImageFolder(pose_data_dir)

# è·å–ç´¢å¼•è¿›è¡Œåˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)
train_idx, val_idx = train_test_split(
    list(range(len(face_full_dataset))),
    test_size=0.2,
    stratify=face_full_dataset.targets,  # ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    random_state=42
)

# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
train_dataset = FusionDataset(
    Subset(face_full_dataset, train_idx),
    Subset(pose_full_dataset, train_idx),
    face_transform=data_transforms['face_train'],
    pose_transform=data_transforms['pose_train']
)
val_dataset = FusionDataset(
    Subset(face_full_dataset, val_idx),
    Subset(pose_full_dataset, val_idx),
    face_transform=data_transforms['face_val'],
    pose_transform=data_transforms['pose_val']
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

# --- 5. æ„å»ºèåˆæ¨¡å‹ --- 
class FusionResNet(nn.Module):
    def __init__(self, num_classes=5):
        super(FusionResNet, self).__init__()
        
        # é¢éƒ¨åˆ†æ”¯
        self.face_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.face_backbone.fc = nn.Identity()
        
        # è‚¢ä½“åˆ†æ”¯
        self.pose_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.pose_backbone.fc = nn.Identity()
        
        # è·å–ç‰¹å¾ç»´åº¦
        self.feature_dim = self.face_backbone.fc.in_features
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.feature_dim * 2, 512),
            nn.ReLU(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # èåˆåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(self.feature_dim * 2, num_classes)
        )
    
    def forward(self, face_x, pose_x):
        # æå–ç‰¹å¾
        face_feat = self.face_backbone(face_x)
        pose_feat = self.pose_backbone(pose_x)
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([face_feat, pose_feat], dim=1)
        
        # æ³¨æ„åŠ›åŠ æƒ
        attention_weights = self.attention(combined)
        face_attn = attention_weights[:, 0].unsqueeze(1) * face_feat
        pose_attn = attention_weights[:, 1].unsqueeze(1) * pose_feat
        
        # åŠ æƒèåˆ
        fused = torch.cat([face_attn, pose_attn], dim=1)
        
        # åˆ†ç±»
        output = self.classifier(fused)
        
        return output

print(f"æ­£åœ¨åŠ è½½èåˆæ¨¡å‹å¹¶è¿è¡Œåœ¨: {device}")
model = FusionResNet(num_classes=num_classes)
model = model.to(device)

# --- 6. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)

# --- 7. è®­ç»ƒå¾ªç¯ ---
# åˆå§‹åŒ–ç”¨äºè®°å½•ç»˜å›¾æ•°æ®çš„å­—å…¸
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

best_val_acc = 0.0
scaler = GradScaler()  # æ··åˆç²¾åº¦åŠ é€Ÿå™¨

print(f"å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {device}")

patience_counter = 0
early_stop_patience = 10

for epoch in range(num_epochs):
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0

    for face_inputs, pose_inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        face_inputs, pose_inputs, labels = face_inputs.to(device), pose_inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(face_inputs, pose_inputs)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡
        running_loss += loss.item() * face_inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += face_inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0

    with torch.no_grad():
        for face_inputs, pose_inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            face_inputs, pose_inputs, labels = face_inputs.to(device), pose_inputs.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(face_inputs, pose_inputs)
                v_loss = criterion(outputs, labels)

            val_loss += v_loss.item() * face_inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += face_inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val
    scheduler.step(epoch_val_loss)
    
    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')
    
    # --- ä¿å­˜æœ€ä½³æ¨¡å‹ ---
    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc
        patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

        # è½¬æ¢å‡†ç¡®ç‡ä¸ºæ•´æ•°ï¼Œå¦‚ 0.9542 -> 9542
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_fusion_model_acc_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")

    else:
        patience_counter += 1
        print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

    # è§¦å‘æ—©åœ
    if patience_counter >= early_stop_patience:
        print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
        break

# --- ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('fusion_training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
