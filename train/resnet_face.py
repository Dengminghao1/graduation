import glob

import torch
import torch.nn as nn
import torch.optim as optim
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm
import matplotlib.pyplot as plt

# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# --- 1. é…ç½®å‚æ•° ---
data_dir = r"/home/ccnu/Desktop/2021214387_å‘¨å©‰å©·/total/classified_frames"  # ä½ ä¹‹å‰åˆ†ç±»å¥½çš„æ ¹ç›®å½•
batch_size = 256
num_epochs = 50
learning_rate = 0.001
num_classes = 5  # ä½, ç¨ä½, ä¸­æ€§, ç¨é«˜, é«˜
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ---
# ResNet æ ‡å‡†è¾“å…¥æ˜¯ 224x224
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((224, 224)), # ç¡®ä¿ä¸‡ä¸€æœ‰é224çš„å›¾ç‰‡è¿›å…¥
        transforms.CenterCrop(224),    # ä»…ä»…æ˜¯ä»ä¸­å¿ƒæˆªå–ï¼Œä¸è¿›è¡Œéšæœºå˜æ¢
        # transforms.Resize((256, 256)),
        # transforms.RandomResizedCrop(224),
        # transforms.RandomHorizontalFlip(),  # æ•°æ®å¢å¼º
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet æ ‡å‡†æ ‡å‡†åŒ–
    ]),
    'val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

# --- 3. åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---
full_dataset = datasets.ImageFolder(data_dir)

# è·å–ç´¢å¼•è¿›è¡Œåˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2,
    stratify=full_dataset.targets,  # ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    random_state=42
)

train_dataset = Subset(full_dataset, train_idx)


# ä¿®æ­£ï¼šä¸ºè®­ç»ƒå’ŒéªŒè¯åˆ›å»ºç‹¬ç«‹çš„å®ä¾‹ä»¥åº”ç”¨ä¸åŒçš„ Transform
class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform

    def __getitem__(self, index):
        x, y = self.subset[index]
        if self.transform: x = self.transform(x)
        return x, y

    def __len__(self):
        return len(self.subset)


train_loader = DataLoader(ApplyTransform(Subset(full_dataset, train_idx), data_transforms['train']),
                          batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(ApplyTransform(Subset(full_dataset, val_idx), data_transforms['val']),
                        batch_size=batch_size, shuffle=False, num_workers=4)

# --- 4. æ„å»º ResNet æ¨¡å‹ ---
print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒ ResNet50 å¹¶è¿è¡Œåœ¨: {device}")
model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)

# ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚ä»¥åŒ¹é…ä½ çš„ 5 åˆ†ç±»
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, num_classes)
model = model.to(device)

# --- 5. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# --- 6. è®­ç»ƒå¾ªç¯ ---
# åˆå§‹åŒ–ç”¨äºè®°å½•ç»˜å›¾æ•°æ®çš„å­—å…¸
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

best_val_acc = 0.0
scaler = GradScaler()  # 4090 æ··åˆç²¾åº¦åŠ é€Ÿå™¨

print(f"å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {device}")

for epoch in range(num_epochs):
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0

    for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # 4090 æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡
        running_loss += loss.item() * inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            inputs, labels = inputs.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                v_loss = criterion(outputs, labels)

            val_loss += v_loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val

    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')

    # --- 3. ä¿å­˜æœ€ä½³æ¨¡å‹ (æ–‡ä»¶åä¸è¦ 0.) ---
    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc

        # æ¸…é™¤æ—§çš„ best æ¨¡å‹
        for old_file in glob.glob("best_model_acc_*.pth"):
            os.remove(old_file)

        # è½¬æ¢å‡†ç¡®ç‡ä¸ºæ•´æ•°ï¼Œå¦‚ 0.9542 -> 9542
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_model_acc_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")

# --- 4. ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
