import glob
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from torch import autocast
from torch.cuda.amp import GradScaler
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset, Dataset, random_split
from sklearn.model_selection import train_test_split
import os
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt

# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# --- 1. é…ç½®å‚æ•° ---
face_data_dir = r"/home/ccnu/Desktop/dataset/extracted_frames_pic/face_extracted_frames_all"  # é¢éƒ¨æ•°æ®
pose_data_dir = r"/home/ccnu/Desktop/dataset/extracted_frames_pic/pose_extracted_frames_all"  # è‚¢ä½“æ•°æ®
csv_dir = r"/home/ccnu/Desktop/dataset/eeg_csv"  # æ ‡ç­¾CSVæ–‡ä»¶ç›®å½•

# face_data_dir = r"D:\dataset\frame_picture\face_extracted_frames_101"  # é¢éƒ¨æ•°æ®
# pose_data_dir = r"D:\dataset\frame_picture\pose_extracted_frames_101"  # è‚¢ä½“æ•°æ®
# csv_dir = r"D:\dataset\eeg_csv"  # æ ‡ç­¾CSVæ–‡ä»¶ç›®å½•

batch_size = 32  # è¿›ä¸€æ­¥å‡å°ä»¥é€‚åº”åºåˆ—è¾“å…¥
num_epochs = 100
learning_rate = 0.0001
num_classes = 5  # ä½, ç¨ä½, ä¸­æ€§, ç¨é«˜, é«˜
sequence_length = 10  # å¸§åºåˆ—é•¿åº¦ï¼ˆçª—å£å¤§å°ä¸ºåï¼‰
hidden_size = 512  # LSTM éšè—å±‚å¤§å°
num_layers = 2  # LSTM å±‚æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# --- MultiSegmentAttentionDataset ç±» --- 
class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_dir, seq_len=10, transform=None, segment_keys=None, is_pose=False):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform
        self.label_map = {'ä½': 0, 'ç¨ä½': 1, 'ä¸­æ€§': 2, 'ç¨é«˜': 3, 'é«˜': 4}
        self.is_pose = is_pose

        # 1. æ‰«æå¹¶åŠ è½½ CSV æ ‡ç­¾åº“
        # key: (start_time_str, end_time_str), value: DataFrame
        self.label_dfs = {}
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        print(f"æ­£åœ¨åŠ è½½ {len(csv_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶...")

        for cf in csv_files:
            # å‡è®¾ CSV æ–‡ä»¶åæ ¼å¼: xxxx_xxxx_20231229153000_20231229154000.csv
            parts = cf.replace('.csv', '').split('_')
            # æ ¹æ®ä½ çš„æ–‡ä»¶åè§„åˆ™ï¼Œå€’æ•°ç¬¬äºŒå’Œå€’æ•°ç¬¬ä¸€é€šå¸¸æ˜¯æ—¶é—´
            s_str, e_str = parts[-2], parts[-1]

            df = pd.read_csv(os.path.join(csv_dir, cf))
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            self.label_dfs[(s_str, e_str)] = df

        # 2. è§£æå›¾åƒæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ®µåˆ†ç»„
        segments = defaultdict(list)
        if self.is_pose:
            all_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]
        else:
            all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]
        print(f"æ­£åœ¨è§£æ {len(all_files)} ä¸ªå›¾åƒæ–‡ä»¶...")

        for f in all_files:
            parts = f.split('_')
            if self.is_pose:
                # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                if len(parts) >= 6:
                    frame_idx = int(parts[-2])
                    s_time_str = parts[-4]
                    e_time_str = parts[-3]
            else:
                # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                if len(parts) >= 5:
                    frame_idx = int(parts[1])
                    s_time_str = parts[-2]
                    e_time_str = parts[-1].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            total_milliseconds = frame_idx * 100  # 0.1ç§’ = 100æ¯«ç§’
            curr_dt = start_dt + timedelta(milliseconds=total_milliseconds)
            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. å¦‚æœæŒ‡å®šäº† segment_keysï¼Œåˆ™åªä¿ç•™è¿™äº›æ®µçš„æ•°æ®
        if segment_keys is not None:
            filtered_segments = {k: v for k, v in segments.items() if k in segment_keys}
        else:
            filtered_segments = segments

        # 4. æ„å»ºåºåˆ—å¹¶ä»"å¯¹åº”"çš„ DataFrame ä¸­å–æ ‡ç­¾
        self.valid_sequences = []
        print("å¼€å§‹æ—¶åºåŒ¹é…æ ‡ç­¾...")

        for seg_key, seg_files_list in filtered_segments.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ CSV æ ‡ç­¾
            if seg_key not in self.label_dfs:
                print(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°æ®µ {seg_key} å¯¹åº”çš„ CSV æ ‡ç­¾ï¼Œè·³è¿‡...")
                continue

            current_df = self.label_dfs[seg_key]
            seg_files = sorted(seg_files_list, key=lambda x: x['idx'])

            for i in range(0, len(seg_files) - seq_len + 1, 10):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # ä½¿ç”¨æœ€åä¸€å¸§çš„æ ‡ç­¾
                end_frame_time = seq_frames[-1]['time']
                time_diffs = (current_df['timestamp'] - end_frame_time).abs()
                closest_idx = time_diffs.idxmin()
                label_str = current_df.loc[closest_idx, 'attention']
                
                self.valid_sequences.append({
                    'files': [x['filename'] for x in seq_frames],
                    'label': self.label_map.get(label_str, 2)
                })

        print(f"æˆåŠŸæ„å»ºåºåˆ—æ€»æ•°: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            else:
                # å¦‚æœæ²¡æœ‰transformï¼Œè‡³å°‘è½¬æ¢ä¸ºtensor
                from torchvision import transforms
                img = transforms.ToTensor()(img)
            frames.append(img)
        # è¿”å›å¸§åºåˆ—å’Œæ ‡ç­¾
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long)



# --- 2. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ---
# ResNet æ ‡å‡†è¾“å…¥æ˜¯ 224x224
# ä½¿ç”¨åŸæœ‰å‚æ•°ï¼šé¢éƒ¨å’Œè‚¢ä½“åˆ†åˆ«ä½¿ç”¨å„è‡ªçš„æ ‡å‡†åŒ–å‚æ•°
data_transforms = {
    'face_train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'face_val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'pose_train': transforms.Compose([
        transforms.Resize(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(5),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.03, 0.03),
            scale=(0.98, 1.02)
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
    'pose_val': transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ]),
}

# --- 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» ---
# åº”ç”¨å˜æ¢çš„æ•°æ®é›†ç±»
class ApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, transform=None):
        self.dataset = dataset
        self.indices = indices
        self.transform = transform

    def __getitem__(self, index):
        img_path, target = self.dataset.samples[self.indices[index]], self.dataset.targets[self.indices[index]]
        from PIL import Image
        img = Image.open(img_path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, target

    def __len__(self):
        return len(self.indices)

# --- 4. è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å™¨ --- 
class FusionDataset(torch.utils.data.Dataset):
    def __init__(self, face_data_dir, pose_data_dir, csv_dir, sequence_length=10):
        self.sequence_length = sequence_length
        self.samples = []
        self.targets = []
        
        # åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®
        face_dataset = MultiSegmentAttentionDataset(face_data_dir, csv_dir, sequence_length, is_pose=False)
        pose_dataset = MultiSegmentAttentionDataset(pose_data_dir, csv_dir, sequence_length, is_pose=True)
        
        # æŒ‰åºåŒ¹é…é¢éƒ¨å’Œè‚¢ä½“æ ·æœ¬
        matched_count = 0
        
        # éå†é¢éƒ¨åºåˆ—
        for face_seq in face_dataset.valid_sequences:
            # è·å–é¢éƒ¨åºåˆ—çš„æ—¶é—´åŒºé—´å’Œå¸§å·
            first_face_img = face_seq['files'][0]
            face_parts = first_face_img.split('_')
            if len(face_parts) >= 5:
                # é¢éƒ¨æ ¼å¼ï¼šframe_000000_192.168.0.101_01_20231229153000_20231229154000.jpg
                face_interval = f"{face_parts[-2]}_{face_parts[-1].split('.')[0]}"
                face_frame_num = int(face_parts[1])
                
                # åœ¨è‚¢ä½“åºåˆ—ä¸­æŸ¥æ‰¾åŒ¹é…çš„åºåˆ—
                for pose_seq in pose_dataset.valid_sequences:
                    first_pose_img = pose_seq['files'][0]
                    pose_parts = first_pose_img.split('_')
                    if len(pose_parts) >= 6:
                        # è‚¢ä½“æ ¼å¼ï¼š192.168.0.101_01_20231229153000_20231229154000_000000002190_rendered.png
                        pose_interval = f"{pose_parts[-4]}_{pose_parts[-3]}"
                        pose_frame_num = int(pose_parts[-2])
                        
                        # æ£€æŸ¥æ—¶é—´åŒºé—´å’Œå¸§å·æ˜¯å¦åŒ¹é…
                        if face_interval == pose_interval and abs(face_frame_num - pose_frame_num) == 0:
                            # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
                            if len(face_seq['files']) == sequence_length and len(pose_seq['files']) == sequence_length:
                                self.samples.append((face_seq['files'], pose_seq['files']))
                                self.targets.append(face_seq['label'])
                                matched_count += 1
                                break
        
        print(f"æˆåŠŸåŒ¹é… {matched_count} å¯¹åºåˆ—æ ·æœ¬")
    
    def __getitem__(self, index):
        face_img_paths, pose_img_paths = self.samples[index]
        target = self.targets[index]
        
        from PIL import Image
        face_imgs = []
        pose_imgs = []
        
        # åŠ è½½åºåˆ—ä¸­çš„æ‰€æœ‰å›¾åƒ
        for face_path, pose_path in zip(face_img_paths, pose_img_paths):
            face_img = Image.open(face_path).convert('RGB')
            pose_img = Image.open(pose_path).convert('RGB')
            face_imgs.append(face_img)
            pose_imgs.append(pose_img)
        
        return face_imgs, pose_imgs, target
    
    def __len__(self):
        return len(self.samples)

# åº”ç”¨å˜æ¢çš„èåˆæ•°æ®é›†ç±»
class FusionApplyTransform(torch.utils.data.Dataset):
    def __init__(self, dataset, indices, face_transform=None, pose_transform=None):
        self.dataset = dataset
        self.indices = indices
        self.face_transform = face_transform
        self.pose_transform = pose_transform

    def __getitem__(self, index):
        face_imgs, pose_imgs, target = self.dataset[self.indices[index]]
        transformed_face_imgs = []
        transformed_pose_imgs = []
        
        # å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨å˜æ¢
        for face_img, pose_img in zip(face_imgs, pose_imgs):
            if self.face_transform:
                face_img = self.face_transform(face_img)
            if self.pose_transform:
                pose_img = self.pose_transform(pose_img)
            transformed_face_imgs.append(face_img)
            transformed_pose_imgs.append(pose_img)
        
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œç»´åº¦ä¸º (åºåˆ—é•¿åº¦, é€šé“, é«˜åº¦, å®½åº¦)
        transformed_face_imgs = torch.stack(transformed_face_imgs)
        transformed_pose_imgs = torch.stack(transformed_pose_imgs)
        
        return transformed_face_imgs, transformed_pose_imgs, target

    def __len__(self):
        return len(self.indices)

# --- 5. åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---  
print("æ­£åœ¨åŠ è½½é¢éƒ¨å’Œè‚¢ä½“æ•°æ®é›†...")

# åˆ›å»ºå®Œæ•´çš„èåˆæ•°æ®é›†ï¼ˆå·²åŒ¹é…çš„æ ·æœ¬ï¼‰
full_dataset = FusionDataset(face_data_dir, pose_data_dir, csv_dir, sequence_length=sequence_length)

# è·å–ç´¢å¼•è¿›è¡Œåˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)
train_idx, val_idx = train_test_split(
    list(range(len(full_dataset))),
    test_size=0.2,
    stratify=full_dataset.targets,  # ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    random_state=42
)

# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
train_dataset = FusionApplyTransform(
    full_dataset,
    train_idx,
    face_transform=data_transforms['face_train'],
    pose_transform=data_transforms['pose_train']
)
val_dataset = FusionApplyTransform(
    full_dataset,
    val_idx,
    face_transform=data_transforms['face_val'],
    pose_transform=data_transforms['pose_val']
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

# --- 5. æ„å»ºèåˆæ¨¡å‹ --- 
class FusionResNetLSTM(nn.Module):
    def __init__(self, num_classes=5, sequence_length=5, hidden_size=512, num_layers=2):
        super(FusionResNetLSTM, self).__init__()
        
        # é¢éƒ¨åˆ†æ”¯
        self.face_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        # ä¿å­˜åŸå§‹ fc å±‚çš„ in_features
        self.feature_dim = self.face_backbone.fc.in_features
        self.face_backbone.fc = nn.Identity()
        
        # è‚¢ä½“åˆ†æ”¯
        self.pose_backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.pose_backbone.fc = nn.Identity()
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.feature_dim * 2, 512),
            nn.ReLU(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # LSTM å±‚
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¾“å…¥åˆ° LSTM çš„ç‰¹å¾ç»´åº¦æ˜¯èåˆåçš„ç‰¹å¾ç»´åº¦
        self.lstm = nn.LSTM(
            input_size=self.feature_dim * 2,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.5
        )
        
        # èåˆåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, face_x, pose_x):
        # è¾“å…¥ç»´åº¦: (batch_size, sequence_length, channels, height, width)
        batch_size = face_x.size(0)
        sequence_length = face_x.size(1)
        
        # è°ƒæ•´ç»´åº¦ä»¥é€‚åº” ResNet: (batch_size * sequence_length, channels, height, width)
        face_x_reshaped = face_x.view(-1, face_x.size(2), face_x.size(3), face_x.size(4))
        pose_x_reshaped = pose_x.view(-1, pose_x.size(2), pose_x.size(3), pose_x.size(4))
        
        # æå–ç‰¹å¾
        face_feat = self.face_backbone(face_x_reshaped)
        pose_feat = self.pose_backbone(pose_x_reshaped)
        
        # è°ƒæ•´ç‰¹å¾ç»´åº¦: (batch_size, sequence_length, feature_dim)
        face_feat = face_feat.view(batch_size, sequence_length, -1)
        pose_feat = pose_feat.view(batch_size, sequence_length, -1)
        
        # ç‰¹å¾èåˆä¸æ³¨æ„åŠ›åŠ æƒ
        fused_features = []
        for t in range(sequence_length):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾
            face_feat_t = face_feat[:, t, :]
            pose_feat_t = pose_feat[:, t, :]
            
            # ç‰¹å¾èåˆ
            combined = torch.cat([face_feat_t, pose_feat_t], dim=1)
            
            # æ³¨æ„åŠ›åŠ æƒ
            attention_weights = self.attention(combined)
            face_attn = attention_weights[:, 0].unsqueeze(1) * face_feat_t
            pose_attn = attention_weights[:, 1].unsqueeze(1) * pose_feat_t
            
            # åŠ æƒèåˆ
            fused = torch.cat([face_attn, pose_attn], dim=1)
            fused_features.append(fused.unsqueeze(1))
        
        # å †å æ‰€æœ‰æ—¶é—´æ­¥çš„èåˆç‰¹å¾: (batch_size, sequence_length, feature_dim * 2)
        fused_sequence = torch.cat(fused_features, dim=1)
        
        # LSTM å¤„ç†
        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(face_x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(face_x.device)
        
        # å‰å‘ä¼ æ’­é€šè¿‡ LSTM
        lstm_out, (hn, cn) = self.lstm(fused_sequence, (h0, c0))
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œåˆ†ç±»
        output = self.classifier(hn[-1])
        
        return output

print(f"æ­£åœ¨åŠ è½½èåˆæ¨¡å‹å¹¶è¿è¡Œåœ¨: {device}")
model = FusionResNetLSTM(
    num_classes=num_classes,
    sequence_length=sequence_length,
    hidden_size=hidden_size,
    num_layers=num_layers
)
model = model.to(device)

# --- 6. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)

# --- 7. è®­ç»ƒå¾ªç¯ ---
# åˆå§‹åŒ–ç”¨äºè®°å½•ç»˜å›¾æ•°æ®çš„å­—å…¸
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

best_val_acc = 0.0
scaler = GradScaler()  # æ··åˆç²¾åº¦åŠ é€Ÿå™¨

print(f"å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {device}")

patience_counter = 0
early_stop_patience = 10

for epoch in range(num_epochs):
    # --- 1. è®­ç»ƒé˜¶æ®µ ---
    model.train()
    running_loss = 0.0
    corrects = 0
    total_train = 0

    for face_inputs, pose_inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
        face_inputs, pose_inputs, labels = face_inputs.to(device), pose_inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast(device_type='cuda'):
            outputs = model(face_inputs, pose_inputs)
            loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ç»Ÿè®¡
        running_loss += loss.item() * face_inputs.size(0)
        _, preds = torch.max(outputs, 1)
        corrects += torch.sum(preds == labels.data)
        total_train += face_inputs.size(0)

    epoch_train_loss = running_loss / total_train
    epoch_train_acc = corrects.double() / total_train

    # --- 2. éªŒè¯é˜¶æ®µ ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    total_val = 0

    with torch.no_grad():
        for face_inputs, pose_inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Val]"):
            face_inputs, pose_inputs, labels = face_inputs.to(device), pose_inputs.to(device), labels.to(device)

            with autocast(device_type='cuda'):
                outputs = model(face_inputs, pose_inputs)
                v_loss = criterion(outputs, labels)

            val_loss += v_loss.item() * face_inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            total_val += face_inputs.size(0)

    epoch_val_loss = val_loss / total_val
    epoch_val_acc = val_corrects.double() / total_val
    scheduler.step(epoch_val_loss)
    
    # è®°å½•æ•°æ®ç”¨äºç»˜å›¾
    history['train_loss'].append(epoch_train_loss)
    history['train_acc'].append(epoch_train_acc.item())
    history['val_loss'].append(epoch_val_loss)
    history['val_acc'].append(epoch_val_acc.item())

    print(f'Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | '  
          f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')

    if epoch_val_acc > best_val_acc:
        best_val_acc = epoch_val_acc
        patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

        # æ¸…é™¤æ—§çš„ best æ¨¡å‹ï¼ˆåªåˆ é™¤å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³çš„ï¼‰
        for old_file in glob.glob("best_model_acc_fusion_lstm_*.pth"):
            # ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
            try:
                old_acc_str = old_file.split('_')[-1].split('.')[0]
                old_acc = int(old_acc_str) / 10000
                # åªæœ‰å½“æ—§æ¨¡å‹çš„å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³å‡†ç¡®ç‡æ—¶æ‰åˆ é™¤
                if old_acc < best_val_acc:
                    os.remove(old_file)
                    print(f"ğŸ”„ åˆ é™¤æ—§æ¨¡å‹: {old_file} (å‡†ç¡®ç‡: {old_acc:.4f})")
            except:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿåˆ é™¤
                os.remove(old_file)
                print(f"ğŸ”„ åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„æ—§æ¨¡å‹: {old_file}")

        # ä¿å­˜æ–°æ¨¡å‹
        acc_suffix = int(best_val_acc * 10000)
        save_path = f'best_model_acc_fusion_lstm_{acc_suffix}.pth'
        torch.save(model.state_dict(), save_path)
        print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")
    else:
        patience_counter += 1
        print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

        # è§¦å‘æ—©åœ
        if patience_counter >= early_stop_patience:
            print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
            break

# --- ç»˜åˆ¶å¹¶ä¿å­˜å›¾åƒ ---
plt.figure(figsize=(12, 5))

# ç»˜åˆ¶ Loss å­å›¾
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# ç»˜åˆ¶ Accuracy å­å›¾
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc', color='blue')
plt.plot(history['val_acc'], label='Val Acc', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('fusion_lstm_training_results.png')  # ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶
plt.show()

print(f'è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
