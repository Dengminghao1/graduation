import glob
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from torch import nn, optim, autocast
from torch.cuda.amp import GradScaler
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from PIL import Image
from datetime import datetime, timedelta
from collections import defaultdict
from tqdm import tqdm
# ç”¨ç¬¬äºŒå—æ˜¾å¡è®­ç»ƒ
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# Quality-aware Temporal Attention LSTM
class QTALSTMCell(nn.Module):
    """
    Quality-aware Temporal Attention LSTM Cell (Confidence-only Version)
    """

    def __init__(self, input_size, hidden_size, use_smooth=True):
        super(QTALSTMCell, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.use_smooth = use_smooth

        # LSTM gates
        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)

        # Learnable decay weight
        self.alpha = nn.Parameter(torch.tensor(1.0))

        # Adaptive decay strength
        self.lambda_param = nn.Parameter(torch.tensor(0.5))

        # For EMA smoothing
        self.register_buffer("conf_prev", torch.zeros(1))


    def forward(self, x_t, h_prev, c_prev, conf_t):
        """
        x_t     : [B, D]
        h_prev  : [B, H]
        c_prev  : [B, H]
        conf_t  : [B]   in [0,1]
        """

        # ---------- 1. Confidence smoothing (optional) ----------
        if self.use_smooth:
            if self.conf_prev.numel() != conf_t.numel():
                self.conf_prev = conf_t.detach()

            conf_s = 0.8 * self.conf_prev + 0.2 * conf_t
            self.conf_prev = conf_s.detach()
        else:
            conf_s = conf_t

        # ---------- 2. Quality-aware delta ----------
        alpha = torch.relu(self.alpha)

        delta_t = alpha * (1 - conf_s)

        # ---------- 3. Exponential decay ----------
        s_t = torch.exp(-delta_t).unsqueeze(1)

        # ---------- 4. Adaptive memory attenuation ----------
        lambda_ = torch.sigmoid(self.lambda_param)

        decay = 1 - lambda_ * (1 - s_t)

        c_prev = c_prev * decay

        # ---------- 5. LSTM gates ----------
        combined = torch.cat([x_t, h_prev], dim=1)

        gates = self.W(combined)

        f_t, i_t, o_t, g_t = gates.chunk(4, dim=1)

        f_t = torch.sigmoid(f_t)
        i_t = torch.sigmoid(i_t)
        o_t = torch.sigmoid(o_t)
        g_t = torch.tanh(g_t)

        # ---------- 6. Input modulation ----------
        i_t = i_t * s_t

        # ---------- 7. Cell update ----------
        c_t = f_t * c_prev + i_t * g_t

        h_t = o_t * torch.tanh(c_t)

        return h_t, c_t

class QTALSTM(nn.Module):

    def __init__(self, input_size, hidden_size, use_smooth=True):
        super(QTALSTM, self).__init__()

        self.hidden_size = hidden_size

        self.cell = QTALSTMCell(
            input_size,
            hidden_size,
            use_smooth
        )


    def forward(self, x, conf):
        """
        x    : [B, T, D]
        conf : [B, T]
        """

        B, T, _ = x.size()

        h = torch.zeros(B, self.hidden_size, device=x.device)
        c = torch.zeros(B, self.hidden_size, device=x.device)

        outputs = []

        for t in range(T):

            h, c = self.cell(
                x[:, t],
                h,
                c,
                conf[:, t]
            )

            outputs.append(h.unsqueeze(1))

        return torch.cat(outputs, dim=1)


class MultiSegmentAttentionDataset(Dataset):
    def __init__(self, img_dir, csv_dir, conf_csv_path=None, seq_len=20, transform=None, segment_keys=None):
        self.img_dir = img_dir
        self.seq_len = seq_len
        self.transform = transform
        self.label_map = {'ä½': 0, 'ç¨ä½': 1, 'ä¸­æ€§': 2, 'ç¨é«˜': 3, 'é«˜': 4}

        # 1. æ‰«æå¹¶åŠ è½½ CSV æ ‡ç­¾åº“
        # key: (start_time_str, end_time_str), value: DataFrame
        self.label_dfs = {}
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
        print(f"æ­£åœ¨åŠ è½½ {len(csv_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶...")

        for cf in csv_files:
            # å‡è®¾ CSV æ–‡ä»¶åæ ¼å¼: xxxx_xxxx_20231229153000_20231229154000.csv
            parts = cf.replace('.csv', '').split('_')
            # æ ¹æ®ä½ çš„æ–‡ä»¶åè§„åˆ™ï¼Œå€’æ•°ç¬¬äºŒå’Œå€’æ•°ç¬¬ä¸€é€šå¸¸æ˜¯æ—¶é—´
            s_str, e_str = parts[-2], parts[-1]

            df = pd.read_csv(os.path.join(csv_dir, cf))
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            self.label_dfs[(s_str, e_str)] = df
            
        # 2. åŠ è½½ç½®ä¿¡åº¦ CSV æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        self.conf_df = None
        if conf_csv_path:
            print(f"æ­£åœ¨åŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶: {conf_csv_path}...")
            try:
                # ç›´æ¥åŠ è½½å•ä¸ªç½®ä¿¡åº¦CSVæ–‡ä»¶
                self.conf_df = pd.read_csv(conf_csv_path,usecols=["timestamp_pose", "confidence"])
                self.conf_df['timestamp'] = pd.to_datetime(self.conf_df['timestamp_pose'])
                print(f"æˆåŠŸåŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶ï¼Œå…± {len(self.conf_df)} æ¡è®°å½•")
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½ç½®ä¿¡åº¦æ–‡ä»¶å¤±è´¥: {e}")

        # 2. è§£æå›¾åƒæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ®µåˆ†ç»„
        segments = defaultdict(list)
        all_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]
        for f in all_files:
            parts = f.split('_')
            # å‡è®¾å›¾åƒæ–‡ä»¶å: xxxx_frameidx_xxxx_xxxx_å¼€å§‹æ—¶é—´_ç»“æŸæ—¶é—´.jpg
            # è¯·æ ¹æ®å®é™…æƒ…å†µç¡®è®¤ parts çš„ç´¢å¼•
            frame_idx = int(parts[1])
            s_time_str = parts[4]
            e_time_str = parts[5].replace('.jpg', '')

            start_dt = datetime.strptime(s_time_str, "%Y%m%d%H%M%S")
            # curr_dt = start_dt + timedelta(seconds=frame_idx * 0.1)
            total_milliseconds = frame_idx * 100  # 0.1ç§’ = 100æ¯«ç§’
            curr_dt = start_dt + timedelta(milliseconds=total_milliseconds)
            segments[(s_time_str, e_time_str)].append({
                'filename': f,
                'time': curr_dt,
                'idx': frame_idx
            })

        # 3. å¦‚æœæŒ‡å®šäº† segment_keysï¼Œåˆ™åªä¿ç•™è¿™äº›æ®µçš„æ•°æ®
        if segment_keys is not None:
            filtered_segments = {k: v for k, v in segments.items() if k in segment_keys}
        else:
            filtered_segments = segments

        # 4. æ„å»ºåºåˆ—å¹¶ä»â€œå¯¹åº”â€çš„ DataFrame ä¸­å–æ ‡ç­¾
        self.valid_sequences = []
        print("å¼€å§‹æ—¶åºåŒ¹é…æ ‡ç­¾...")

        for seg_key, seg_files_list in filtered_segments.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ CSV æ ‡ç­¾
            if seg_key not in self.label_dfs:
                print(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°æ®µ {seg_key} å¯¹åº”çš„ CSV æ ‡ç­¾ï¼Œè·³è¿‡...")
                continue

            current_df = self.label_dfs[seg_key]
            seg_files = sorted(seg_files_list, key=lambda x: x['idx'])

            for i in range(0, len(seg_files) - seq_len,10):
                seq_frames = seg_files[i: i + seq_len]
                end_frame_time = seq_frames[-1]['time']

                # ä¸ºåºåˆ—ä¸­çš„æ¯å¼ å›¾ç‰‡å•ç‹¬è·å–ç½®ä¿¡åº¦
                conf_sequence = []
                valid_sequence = True
                
                # å¯¹äºåºåˆ—ä¸­çš„æ¯å¼ å›¾ç‰‡
                for frame in seq_frames:
                    frame_time = frame['time']
                    
                    # ä»ç½®ä¿¡åº¦CSVæ–‡ä»¶ä¸­è·å–ç½®ä¿¡åº¦
                    if self.conf_df is not None:
                        # åœ¨ç½®ä¿¡åº¦ DataFrame ä¸­æ‰¾æœ€æ¥è¿‘çš„æ—¶é—´ç‚¹
                        time_diffs = (self.conf_df['timestamp'] - frame_time).abs()
                        closest_idx = time_diffs.idxmin()
                        
                        if time_diffs.min() <= timedelta(seconds=1):
                            # å°è¯•è·å– confidence å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ä¸ºé»˜è®¤å€¼ 0.5
                            confidence = self.conf_df.loc[closest_idx].get('confidence', 0.5)
                            conf_sequence.append(confidence)
                        else:
                            # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„æ—¶é—´ç‚¹ï¼Œæ ‡è®°åºåˆ—ä¸ºæ— æ•ˆ
                            valid_sequence = False
                            break
                    else:
                        # å¦‚æœæ²¡æœ‰æä¾›ç½®ä¿¡åº¦æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5
                        confidence = 0.5
                        conf_sequence.append(confidence)
                
                # å¦‚æœæ‰€æœ‰å¸§éƒ½æ‰¾åˆ°äº†æœ‰æ•ˆçš„ç½®ä¿¡åº¦ï¼Œæ‰ä¿å­˜è¯¥åºåˆ—
                if valid_sequence and conf_sequence:
                    # ä½¿ç”¨æœ€åä¸€å¸§çš„æ ‡ç­¾
                    end_frame_time = seq_frames[-1]['time']
                    time_diffs = (current_df['timestamp'] - end_frame_time).abs()
                    closest_idx = time_diffs.idxmin()
                    label_str = current_df.loc[closest_idx, 'attention']
                    
                    self.valid_sequences.append({
                        'files': [x['filename'] for x in seq_frames],
                        'label': self.label_map.get(label_str, 2),
                        'confidence': conf_sequence
                    })

        print(f"æˆåŠŸæ„å»ºåºåˆ—æ€»æ•°: {len(self.valid_sequences)}")

    def __len__(self):
        return len(self.valid_sequences)

    def __getitem__(self, idx):
        data = self.valid_sequences[idx]
        frames = []
        for fname in data['files']:
            img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
            if self.transform:
                img = self.transform(img)
            frames.append(img)
        # è¿”å›å¸§åºåˆ—ã€æ ‡ç­¾å’Œç½®ä¿¡åº¦åºåˆ—
        return torch.stack(frames), torch.tensor(data['label'], dtype=torch.long), torch.tensor(data['confidence'], dtype=torch.float32)


# ===========================
# 2. æ¨¡å‹å®šä¹‰ (ResNet50 + LSTM)
# ===========================
class ResNet50QTALSTM(nn.Module):
    def __init__(self, num_classes=5, hidden_size=512):
        super(ResNet50QTALSTM, self).__init__()
        weights = models.ResNet50_Weights.IMAGENET1K_V1
        resnet = models.resnet50(weights=weights)

        self.resnet_out_dim = resnet.fc.in_features
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

        # æ–°å¢ï¼šç‰¹å¾æ ‡å‡†åŒ–å±‚ï¼Œé˜²æ­¢ CNN è¾“å‡ºèŒƒå›´æ³¢åŠ¨è¿‡å¤§
        self.bn = nn.BatchNorm1d(self.resnet_out_dim)

        # ä½¿ç”¨ QTALSTM æ›¿ä»£ LSTM
        self.qlstm = QTALSTM(
            input_size=self.resnet_out_dim,
            hidden_size=hidden_size,
            use_smooth=True
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.6),
            nn.Linear(256, num_classes)
        )

    def forward(self, x, conf):
        b, s, c, h, w = x.shape
        x_flat = x.view(b * s, c, h, w)
        features = self.feature_extractor(x_flat)  # (B*S, 2048, 1, 1)

        # å±•å¹³å¹¶æ ‡å‡†åŒ–
        features = features.view(b * s, -1)
        features = self.bn(features)

        # æ¢å¤æ—¶åºç»´åº¦
        features = features.view(b, s, -1)

        # ä½¿ç”¨ QTALSTM å¤„ç†åºåˆ—ï¼Œä½¿ç”¨å¤–éƒ¨ä¼ é€’çš„ç½®ä¿¡åº¦
        qlstm_out = self.qlstm(features, conf)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_timestep_out = qlstm_out[:, -1, :]
        return self.classifier(last_timestep_out)


# ===========================
# 3. é…ç½®ä¸è®­ç»ƒè„šæœ¬
# ===========================
if __name__ == '__main__':
    # --- é…ç½® ---
    IMG_DIR = r'/home/ccnu/Desktop/dataset/extracted_frames_pic/face_extracted_frames_all'  # <-- ä¿®æ”¹è¿™é‡Œ
    CSV_DIR = r'/home/ccnu/Desktop/dataset/eeg_csv'  # <-- ä¿®æ”¹è¿™é‡Œ
    # æ·»åŠ ç½®ä¿¡åº¦CSVæ–‡ä»¶ç›®å½•å‚æ•°
    CONF_CSV_PATH = r'/home/ccnu/Desktop/dataset/confidence.csv'  # <-- ä¿®æ”¹è¿™é‡Œ
    IMG_DIR = r'D:\dataset\frame_picture\face_extracted_frames_101'  # <-- ä¿®æ”¹è¿™é‡Œ
    CSV_DIR = r'D:\dataset\eeg_csv'  # <-- ä¿®æ”¹è¿™é‡Œ
    # æ·»åŠ ç½®ä¿¡åº¦CSVæ–‡ä»¶ç›®å½•å‚æ•°
    CONF_CSV_PATH = r"D:\dataset\Dataset_align_face_pose_eeg_feature.csv" # <-- ä¿®æ”¹è¿™é‡Œ
    # IMG_DIR = r'/home/ccnu/Desktop/dataset/frames_face_all'  # <-- ä¿®æ”¹è¿™é‡Œ
    # CSV_DIR = r'/home/ccnu/Desktop/dataset/eeg_csv'  # <-- ä¿®æ”¹è¿™é‡Œ

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {DEVICE}")

    # é’ˆå¯¹ 4090 çš„è¶…å‚æ•°è®¾ç½®
    BATCH_SIZE = 32  # å¢åŠ æ‰¹é‡å¤§å°ä»¥æé«˜æ³›åŒ–èƒ½åŠ›
    SEQ_LEN = 10  # è¾“å…¥ 1 ç§’çš„è§†é¢‘ (10fps * 3s)
    NUM_EPOCHS = 50  # å¢åŠ è®­ç»ƒè½®æ•°
    LEARNING_RATE = 1e-5  # æ›´å°çš„åˆå§‹å­¦ä¹ ç‡
    NUM_CLASSES = 5

    # å›¾åƒé¢„å¤„ç†
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize(256),
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.ColorJitter(0.2, 0.2, 0.2),
            transforms.RandomGrayscale(p=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    # --- 1. æ•°æ®é›†åˆ‡åˆ†ä¸åŠ è½½ ---
    # å‡è®¾ä½ çš„ MultiSegmentAttentionDataset ç±»å·²ç»åœ¨ä¸Šæ–¹å®šä¹‰å¥½
    print("æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†...")
    # 1. é¦–å…ˆè§£æå‡ºæ‰€æœ‰çš„æ®µ key
    all_files = [f for f in os.listdir(IMG_DIR) if f.endswith('.jpg')]
    temp_segments = set()
    for f in all_files:
        parts = f.split('_')
        temp_segments.add((parts[4], parts[5].replace('.jpg', '')))
    all_keys = list(temp_segments)

    # # 2. æŒ‰â€œæ®µâ€åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯ï¼ˆç¡®ä¿éªŒè¯é›†æ˜¯å…¨æ–°çš„è§†é¢‘æ®µï¼‰
    # train_keys, val_keys = train_test_split(all_keys, test_size=0.2, random_state=42)
    #
    # # 3. å®ä¾‹åŒ–ä¸¤ä¸ªç‹¬ç«‹çš„ Dataset
    # train_dataset = MultiSegmentAttentionDataset(IMG_DIR, CSV_DIR, SEQ_LEN, data_transforms['train'], segment_keys=train_keys)
    # val_dataset = MultiSegmentAttentionDataset(IMG_DIR, CSV_DIR, SEQ_LEN, data_transforms['val'], segment_keys=val_keys)
    #
    # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    # val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    full_dataset = MultiSegmentAttentionDataset(IMG_DIR, CSV_DIR, CONF_CSV_PATH, SEQ_LEN, data_transforms['train'])

    # è·å–æ•°æ®é›†é•¿åº¦å¹¶è¿›è¡Œéšæœºåˆ‡åˆ†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size

    # ä½¿ç”¨random_splitè¿›è¡Œéšæœºåˆ‡åˆ†
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    # --- 2. æ¨¡å‹åˆå§‹åŒ– ---
    model = ResNet50QTALSTM(num_classes=NUM_CLASSES).to(DEVICE)
    # ä¸resnet_face.pyä¿æŒä¸€è‡´
    criterion = nn.CrossEntropyLoss()
    # --- åœ¨åˆå§‹åŒ–ä¼˜åŒ–å™¨åæ·»åŠ  ---
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)  # å¢åŠ æƒé‡è¡°å‡ä»¥å¢å¼ºæ­£åˆ™åŒ–
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)  # å‡å°‘ patience ä»¥æ›´æ¿€è¿›åœ°è¡°å‡å­¦ä¹ ç‡
    scaler = GradScaler()

    # ç”¨äºç»˜å›¾çš„åˆ—è¡¨
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }

    best_val_acc = 0.0
    print(f"å¼€å§‹è®­ç»ƒ... è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")

    patience_counter = 0
    early_stop_patience = 5  # å‡å°‘æ—©åœè€å¿ƒå€¼ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    # --- 3. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(NUM_EPOCHS):
        # --- 1. è®­ç»ƒé˜¶æ®µ (Training Phase) ---
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0

        # ä½¿ç”¨ tqdm åŒ…è£… train_loader
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]")

        for inputs, labels, conf in train_bar:
            inputs, labels, conf = inputs.to(DEVICE), labels.to(DEVICE), conf.to(DEVICE)
            optimizer.zero_grad()

            with autocast(device_type='cuda'):
                outputs = model(inputs, conf)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            # æ·»åŠ æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            # ç»Ÿè®¡æ•°æ®
            current_batch_size = inputs.size(0)
            train_loss += loss.item() * current_batch_size
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡å³ä¾§çš„æ˜¾ç¤ºä¿¡æ¯ (å½“å‰ batch çš„ loss)
            train_bar.set_postfix(loss=f"{loss.item():.4f}")

        avg_train_loss = train_loss / len(train_dataset)
        avg_train_acc = train_correct / train_total

        # --- 2. éªŒè¯é˜¶æ®µ (Validation Phase) ---
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0

        # éªŒè¯é›†ä¹Ÿå»ºè®®åŠ ä¸Šè¿›åº¦æ¡ï¼Œå°¤å…¶å½“éªŒè¯é›†è¾ƒå¤§æ—¶
        val_bar = tqdm(val_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Val]")

        with torch.no_grad():
            for inputs, labels, conf in val_bar:
                inputs, labels, conf = inputs.to(DEVICE), labels.to(DEVICE), conf.to(DEVICE)

                with autocast(device_type='cuda'):
                    outputs = model(inputs, conf)
                    v_loss = criterion(outputs, labels)

                val_loss += v_loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                val_bar.set_postfix(val_loss=f"{v_loss.item():.4f}")

        avg_val_loss = val_loss / len(val_dataset)
        avg_val_acc = val_correct / val_total
        scheduler.step(avg_val_loss)  # è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
        # åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œåœ¨ scheduler.step() åæ·»åŠ 
        if epoch % 1 == 0:  # æ¯éš”ä¸€å®šepochè¾“å‡ºä¸€æ¬¡
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Learning Rate: {current_lr:.2e}")
        # --- 3. ç»“æœè®°å½•ä¸ä¿å­˜ ---
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)

        # æ‰“å°æœ€ç»ˆæ±‡æ€»ç»“æœ
        print(f"\nSummary - Epoch [{epoch + 1}/{NUM_EPOCHS}]: "
              f"Train Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.4f}")

        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

            # æ¸…é™¤æ—§çš„ best æ¨¡å‹ï¼ˆåªåˆ é™¤å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³çš„ï¼‰
            for old_file in glob.glob("best_model_acc_face_qlstm_*.pth"):
                # ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
                try:
                    old_acc_str = old_file.split('_')[-1].split('.')[0]
                    old_acc = int(old_acc_str) / 10000
                    # åªæœ‰å½“æ—§æ¨¡å‹çš„å‡†ç¡®ç‡ä½äºå½“å‰æœ€ä½³å‡†ç¡®ç‡æ—¶æ‰åˆ é™¤
                    if old_acc < best_val_acc:
                        os.remove(old_file)
                        print(f"ğŸ”„ åˆ é™¤æ—§æ¨¡å‹: {old_file} (å‡†ç¡®ç‡: {old_acc:.4f})")
                except:
                    # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿåˆ é™¤
                    os.remove(old_file)
                    print(f"ğŸ”„ åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„æ—§æ¨¡å‹: {old_file}")

            # ä¿å­˜æ–°æ¨¡å‹
            acc_suffix = int(best_val_acc * 10000)
            save_path = f'best_model_acc_face_qlstm_{acc_suffix}.pth'
            torch.save(model.state_dict(), save_path)
            print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {save_path}")
        else:
            patience_counter += 1
            print(f"âš  éªŒè¯é›†è¡¨ç°æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")

            # è§¦å‘æ—©åœ
            if patience_counter >= early_stop_patience:
                print("ğŸ›‘ [Early Stopping] éªŒè¯é›†è¡¨ç°é•¿æœŸåœæ»ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚")
                break

    # --- 4. ç»˜åˆ¶ç»“æœå›¾åƒ ---
    plt.figure(figsize=(12, 5))

    # ç»˜åˆ¶ Loss æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # ç»˜åˆ¶ Accuracy æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Acc')
    plt.plot(history['val_acc'], label='Val Acc')
    plt.title('Training & Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_results_qlstm.png')
    plt.show()

    print(f"è®­ç»ƒç»“æŸ! æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.4f}")